<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-11-13">

<title>Statistical Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-443a8c2d8fa65f4c6b846c90aafe8287.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-30fc45633b0910a22497ea642fc28e10.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-443a8c2d8fa65f4c6b846c90aafe8287.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Statistical Learning</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#learners" id="toc-learners" class="nav-link" data-scroll-target="#learners">Learners</a>
  <ul class="collapse">
  <li><a href="#maximum-likeilhood-estimation-mle" id="toc-maximum-likeilhood-estimation-mle" class="nav-link" data-scroll-target="#maximum-likeilhood-estimation-mle">Maximum Likeilhood Estimation (MLE)</a></li>
  <li><a href="#maximum-a-posteriori-map-estimation" id="toc-maximum-a-posteriori-map-estimation" class="nav-link" data-scroll-target="#maximum-a-posteriori-map-estimation">Maximum a Posteriori (MAP) Estimation</a></li>
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference">Bayesian Inference</a></li>
  <li><a href="#others" id="toc-others" class="nav-link" data-scroll-target="#others">Others</a></li>
  </ul></li>
  <li><a href="#example-mle-for-multi-label-classification" id="toc-example-mle-for-multi-label-classification" class="nav-link" data-scroll-target="#example-mle-for-multi-label-classification">Example: MLE for multi-label classification</a>
  <ul class="collapse">
  <li><a href="#analytical-estimation-of-theta" id="toc-analytical-estimation-of-theta" class="nav-link" data-scroll-target="#analytical-estimation-of-theta">Analytical Estimation of <span class="math inline">\(\theta\)</span></a></li>
  <li><a href="#gradient-descent-estimation-of-theta" id="toc-gradient-descent-estimation-of-theta" class="nav-link" data-scroll-target="#gradient-descent-estimation-of-theta">Gradient Descent Estimation of <span class="math inline">\(\theta\)</span></a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Statistical Learning</h1>
<p class="subtitle lead">A Review on Computer Vision Fundamentals</p>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 13, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level1 page-columns page-full">
<h1>Introduction</h1>
<p>You have probably seen this thing floating around in several machine learning courses: <span class="math inline">\(P(y|x,\theta)\)</span>, <span class="math inline">\(P(\theta|x,y)\)</span>, or even the term MAP and MLE. If you haven’t yet, you will probably start seeing them after. These concepts comes from statistics where they’re called statistical learning, parameter estimation, probabilistic inference, or one of their many synonyms. But what are they? How do they relate to vision? And why is it useful to think about them?</p>
<p>Let us start with a toy example. We want to design an autonomous system to tell when the car should go or stop, depending on the traffic light. Our dataset are pictures of traffic lights and we want our model <span class="math inline">\(M(\theta)\)</span> to classify whether an image is <span class="math inline">\(\text{GO}\)</span> or <span class="math inline">\(\text{STOP}\)</span> parameterized by <span class="math inline">\(\theta\)</span>. Using one-hot encoding, we let <span class="math inline">\(\mathbf{y}_i:=\begin{bmatrix} 1 &amp; 0 \end{bmatrix}^\top\)</span> for <span class="math inline">\(\text{GO}\)</span> and <span class="math inline">\(\mathbf{y}_i:=\begin{bmatrix} 0 &amp; 1 \end{bmatrix}^\top\)</span> for <span class="math inline">\(\text{STOP}\)</span>. Let red and yellow light be <span class="math inline">\(\text{STOP}\)</span> and green light be <span class="math inline">\(\text{GO}\)</span> for our intents and purposes.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../img/blog/20_trafficlight.png" class="img-fluid figure-img" width="500"></p>
<figcaption><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></figcaption>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Image copied from <a href="https://github.com/Alyxion/Udacity_SelfDrivingCarEngineerNd">here</a></p></div></div></figure>
</div>
<p>To simplify our case even more, let us abstract away the high-dimensional input of an image and let <span class="math inline">\(\mathbf{x}_i:=\begin{bmatrix} x_r &amp; x_y &amp; x_g \end{bmatrix}^\top\)</span>, where each component <span class="math inline">\(\in[0,1]\)</span> corresponds to a snapshot of the illumination intensity of red <span class="math inline">\(x_r\)</span>, yellow <span class="math inline">\(x_y\)</span>, and the green <span class="math inline">\(x_g\)</span> bulb of a traffic light. So, we can visualize our data as…</p>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
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
</script>
</div>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBtYXRwbG90bGliIGltcG9ydCBweXBsb3QgYXMgcGx0XG5cbmZpZywgYXhlcyA9IHBsdC5zdWJwbG90cygxLCAzLCBmaWdzaXplPSg3LCA0KSwgc2hhcmV5PVRydWUpXG5cbnByaW50KHhfcmVuZGVyLnNoYXBlKSAgIyBmb3IgcmVuZGVyaW5nIHRoaXMgcGxvdCAoTiB4IDMgeCAxIHggMylcbnByaW50KHguc2hhcGUpICAjIGZvciBkYXRhIChOIHggMylcbmF4ZXNbMF0uaW1zaG93KHhfcmVuZGVyWzBdLCBpbnRlcnBvbGF0aW9uPVwibmVhcmVzdFwiKVxuYXhlc1sxXS5pbXNob3coeF9yZW5kZXJbMjAwXSwgaW50ZXJwb2xhdGlvbj1cIm5lYXJlc3RcIilcbmF4ZXNbMl0uaW1zaG93KHhfcmVuZGVyWzEwMF0sIGludGVycG9sYXRpb249XCJuZWFyZXN0XCIpXG5heGVzWzBdLnNldF90aXRsZSgnU1RPUCcsIGZvbnRzaXplPTI0KVxuYXhlc1sxXS5zZXRfdGl0bGUoJ0dPJywgZm9udHNpemU9MjQpXG5heGVzWzJdLnNldF90aXRsZSgnU1RPUCcsIGZvbnRzaXplPTI0KVxuYXhlc1swXS5heGlzKCdvZmYnKVxuYXhlc1sxXS5heGlzKCdvZmYnKVxuYXhlc1syXS5heGlzKCdvZmYnKVxucGx0LnRpZ2h0X2xheW91dCgpXG5wbHQuc2hvdygpIn0=
</script>
</div>
<p>Now that we have introduced our scenario, let us get into the main idea of this blog.</p>
</section>
<section id="learners" class="level1 page-columns page-full">
<h1>Learners</h1>
<p>For any ML-based vision models (really any vision models you come across now,) we have a learner and a model, as explained in <span class="citation" data-cites="foundationsCVbook">Torralba, Isola, and Freeman (<a href="#ref-foundationsCVbook" role="doc-biblioref">2024, chap. 9</a>)</span>. A model can be neural networks, transformers, etc… but we will do something simpler. But first, what kind of learner are we trying to model here? Is it supervised, self-supervised, unsupervised, or reinforcement learning? Is it a generative or discriminative learning? <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="ref-foundationsCVbook" class="csl-entry" role="listitem">
Torralba, A., P. Isola, and W. T. Freeman. 2024. <em>Foundations of Computer Vision</em>. Adaptive Computation and Machine Learning Series. MIT Press. <a href="https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/">https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/</a>.
</div><div id="fn2"><p><sup>2</sup>&nbsp;After the generative model boom, this distinction has become more important. Generative model has long existed in statistics, and has only been recently (within 5 years) exposed to the vision community at a scale.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Since there are only two categories, we can simplify it to a binary classification; however, multi-class classification generalizes to any <span class="math inline">\(K\)</span> categories, it will be more easier to visualize how it works. Single-label just means the categories are mutually exclusive, and the model should expect to output a single category per input sample (i.e., image).</p></div></div><p>Since we already have a pretty clear <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, it probably means we are working under a supervised learning regime. We want our model to predict <span class="math inline">\(\mathbf{y}\)</span>, so it would also be under supervised discriminative learning regime. Since our labels for <span class="math inline">\(\mathbf{y}\)</span> are categorical, we will specifically be working with multi-class (single-label) classifiers <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>Before continuiting, please refer to <span class="citation" data-cites="mlprobabilistic">Murphy (<a href="#ref-mlprobabilistic" role="doc-biblioref">2012, chap. 3.2</a>)</span> to know more about Bayes’ theorem and how likelihoods, posteriors, priors, and evidence all have special meaning and nuances despite them denoted as <span class="math inline">\(p\)</span> for probabilities.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mlprobabilistic" class="csl-entry" role="listitem">
Murphy, Kevin P. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. The MIT Press.
</div><div id="ref-princeCVMLI2012" class="csl-entry" role="listitem">
Prince, S. J. D. 2012. <em><span class="nocase">Computer Vision: Models Learning and Inference</span></em>. <span>Cambridge University Press</span>.
</div><div id="fn4"><p><sup>4</sup>&nbsp;If you known gradient descent and the notion of “loss” in ML/vision, the following three approaches are a generalization to them. These general form allows us to see learning at a bigger picture, though we won’t be discussing this here.</p></div></div><p>According to <span class="citation" data-cites="princeCVMLI2012">S. J. D. Prince (<a href="#ref-princeCVMLI2012" role="doc-biblioref">2012, chap. 4</a>)</span>, there are three classical approaches for supervised discriminative learning<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>:</p>
<section id="maximum-likeilhood-estimation-mle" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likeilhood-estimation-mle">Maximum Likeilhood Estimation (MLE)</h2>
<p><span class="math inline">\(\hat{\theta}=\arg\max_{\theta}p(\mathbf{y}|\mathbf{x},\theta)\)</span></p>
<p>Remember, <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> are our training data and cannot be changed. The only thing we can (and should) change is the model parameter <span class="math inline">\(\theta\)</span>. So, this is basically saying what value of <span class="math inline">\(\theta\)</span> can maximize the probability of the correct label <span class="math inline">\(\mathbf{y}\)</span> from its corresponding input <span class="math inline">\(\mathbf{x}\)</span> for all samples (i.e., data points). The <span class="math inline">\(p(\mathbf{y}|\mathbf{x},\theta)\)</span> is known as the likelihood of <span class="math inline">\(\theta\)</span>. We will define what <span class="math inline">\(\theta\)</span> is later, but basically it is an abstract representation of model’s parameter, representing choices of hypothesis that best explains data relationship.</p>
</section>
<section id="maximum-a-posteriori-map-estimation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="maximum-a-posteriori-map-estimation">Maximum a Posteriori (MAP) Estimation</h2>
<p><span class="math inline">\(\hat{\theta}=\arg\max_{\theta}p(\theta|\mathbf{x},\mathbf{y})=\arg\max_{\theta}\frac{p(\mathbf{y}|\mathbf{x},\theta)p(\theta)}{p(\mathbf{x},\mathbf{y})}\propto \arg\max_{\theta}p(\mathbf{y}|\mathbf{x},\theta)p(\theta)\)</span></p>
<p>This is saying which parameter <span class="math inline">\(\theta\)</span> for our model (i.e., weights) has the highest probability that explains the data relationship between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. In other words, this maximizes the posterior distribution <span class="math inline">\(P(\theta|\mathbf{x},\mathbf{y})\)</span> (i.e., the posterior of <span class="math inline">\(\theta\)</span>). We then use the Bayes’ theorem <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> to get back our likelihood again with an additional prior <span class="math inline">\(p(\theta)\)</span> <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> with a constant factor <span class="math inline">\(\frac{1}{p(\mathbf{x},\mathbf{y})}\)</span> which can be ignored <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;A commonly used special operation in statistics/probabilities. Check <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">here</a></p></div><div id="fn6"><p><sup>6</sup>&nbsp;There’s many ways to think about prior <span class="math inline">\(p(\theta)\)</span>. You think of it as a weighting factor of the likelihood that says how likely that <em>likeli</em>hood is given the selected <span class="math inline">\(\theta\)</span>. Or, as a balance between data-driven likelihood and previously known (i.e., “prior”) knowledge on how <span class="math inline">\(\theta\)</span> should behave (e.g., <span class="math inline">\(\theta\)</span> is likely to be 67 for some reason). Or, as a regularizer where we follow Occam’s razor that the weights should be simple.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;This is a probability over the training data <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{x}\)</span>. It’s not going to change throughout the maximization process of <span class="math inline">\(\theta\)</span>.</p></div></div></section>
<section id="bayesian-inference" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bayesian-inference">Bayesian Inference</h2>
<p><span class="math inline">\(p(y^*|x^*,\mathbf{x},\mathbf{y})=\int p(y^*|x^*,\theta)p(\theta|\mathbf{x},\mathbf{y})d\theta\)</span></p>
<p>Instead of estimating a specific parameter <span class="math inline">\(\theta\)</span> (i.e., point estimate), we interpret the model’s parameters <span class="math inline">\(\theta\)</span> as probabilistic <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Notice we are not maximizing anything or even calculating <span class="math inline">\(\theta\)</span> itself, but directly predicting our unseen data point <span class="math inline">\(y^*\)</span> given <span class="math inline">\(x^*\)</span>. In fact, this is a summation of each prediction <span class="math inline">\(p(y^*|x^*,\theta)\)</span> from all possible parameter <span class="math inline">\(\theta\)</span> weighted by the posterior for each <span class="math inline">\(\theta\)</span>. The integral can sometimes be computed analytically given a good conjugate prior <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, but most of the time, it is approximated by sampling (i.e., Monte-Carlo).</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;In statistics, there are two interpretations: <a href="https://en.wikipedia.org/wiki/Frequentist_probability">frequentist</a> (there’s a single true value and the randomness is strictly from sampling error like MLE/MAP) and <a href="https://en.wikipedia.org/wiki/Bayesian_probability">Bayesian</a> (there are no true value and the probability is intrinsic).</p></div><div id="fn9"><p><sup>9</sup>&nbsp;A special prior when combined with its corresponding likelihood produces a nice-to-work-with analytical posterior (remember, we are integrating the posterior, which is nasty most of the time). For example, the Gaussian distribution <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>’s conjugate prior is Normal-inverse gamma. Check <a href="https://en.wikipedia.org/wiki/Conjugate_prior">here</a>.</p></div></div></section>
<section id="others" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="others">Others</h2>
<p>For generative or unsupervised models, we don’t necessarily have a label <span class="math inline">\(\mathbf{y}\)</span>. What we do instead is have the model learn the data distribution of the input <span class="math inline">\(\mathbf{x}\)</span> itself, resulting in posteriors with <span class="math inline">\(p(\theta|\mathbf{x})\)</span> instead of <span class="math inline">\(p(\theta|\mathbf{x},\mathbf{y})\)</span>, or likelihood of <span class="math inline">\(p(\mathbf{x}|\theta)\)</span> instead of <span class="math inline">\(p(\mathbf{y}|\mathbf{x},\theta)\)</span>. Sometimes, they also have the latent distribution <span class="math inline">\(\mathbf{z}\)</span> as a more efficient, intermediate way to have the model learn the distribution. For example, VAEs <span class="citation" data-cites="prince2023understanding">Simon J. D. Prince (<a href="#ref-prince2023understanding" role="doc-biblioref">2023, chap. 17.3</a>)</span> and diffusions <span class="citation" data-cites="prince2023understanding">Simon J. D. Prince (<a href="#ref-prince2023understanding" role="doc-biblioref">2023, chap. 18.4</a>)</span> are learned via MLE with <span class="math inline">\(p(\mathbf{x}|\theta)\)</span> as the likelihood. In some cases, we do a maximization and a minimization of two different sets of parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> like in GANs <span class="citation" data-cites="prince2023understanding">Simon J. D. Prince (<a href="#ref-prince2023understanding" role="doc-biblioref">2023, chap. 15.1.1</a>)</span>. Or, in Deep RL, we maximize the policy <span class="citation" data-cites="prince2023understanding">Simon J. D. Prince (<a href="#ref-prince2023understanding" role="doc-biblioref">2023, chap. 19.3</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>For the purpose of learning (i.e., you learning this), we will stick with much more simpler MLE under supervised discriminative learning. Now enough with the theory and see some visuals from our example scenario.</p>
</section>
</section>
<section id="example-mle-for-multi-label-classification" class="level1 page-columns page-full">
<h1>Example: MLE for multi-label classification</h1>
<p>Let us first visualize our data.</p>
<div>
<div id="pyodide-3" class="exercise-cell">

</div>
<script type="pyodide-3-contents">
eyJhdHRyIjp7ImVjaG8iOmZhbHNlLCJlZGl0IjpmYWxzZSwiZXZhbCI6dHJ1ZX0sImNvZGUiOiJcbnByaW50KFwieCBkaW1lbnNpb246XCIsIHguc2hhcGUpICAjIE4geCAzXG5wcmludChcInkgZGltZW5zaW9uOlwiLCB5LnNoYXBlKSAgIyBOIHggMlxuXG4jIENyZWF0ZSBtYXNrcyBmb3IgR08vU1RPUCBjbGFzc2VzIChyZXVzZWQgdGhyb3VnaG91dClcbnN0b3BfbWFzayA9IHlbOiwgMV0gPT0gMVxuZ29fbWFzayA9IHlbOiwgMF0gPT0gMVxuXG4jIFVzZSBtb2R1bGFyIHBsb3R0aW5nIGZ1bmN0aW9uXG5maWcsIGF4ID0gcGxvdF8zZF90cmFmZmljX2RhdGEoeCwgeSlcbnBsdC5zaG93KCkifQ==
</script>
</div>
<p>Our likelihood <span class="math inline">\(p(\mathbf{y}|\mathbf{x},\theta)\)</span> will follow a categorical distribution where <span class="math inline">\(p(\mathbf{y}_i=\mathbf{k}|\mathbf{x}_i,\theta)\)</span> denotes the likelihood for each data sample <span class="math inline">\(i\)</span> and <span class="math inline">\(\mathbf{k}\in K=\{\text{STOP},\text{GO}\}\)</span>, where <span class="math inline">\(K\)</span> can be thought of as a set of all one-hot-encoded categories. <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;Think of it like a vector-valued likelihood where <span class="math inline">\(p(\mathbf{y}_i|\mathbf{x}_i,\theta)=\begin{bmatrix} p(\mathbf{y}_i= &amp; \text{GO} &amp; |\mathbf{x}_i,\theta) \\ p(\mathbf{y}_i= &amp; \text{STOP} &amp; |\mathbf{x}_i,\theta) \end{bmatrix}\)</span></p></div></div><p>Now, to choose a model, we will use multivariate Gaussian distribution to represent each categories. Remember, the model we choose are arbitrary and mostly follows what we think is the best for the given situation. We might choose logistic regression for a binary classification (which is still slightly different from two-label classification) to decide whether an image of a light is <span class="math inline">\(\text{GO}\)</span> or <span class="math inline">\(\text{STOP}\)</span>. Or, we can follow one of the more later trends and use neural networks on everything. But overall, it doesn’t change the overall theory of learning and the underlying optimization and criterion/loss, so we pick the simpler option: multivariate Gaussian distribution.</p>
<p>Recall, a univariate Gaussian <span class="math inline">\(\mathcal{N}(x|\mu, \sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}=\frac{1}{\sqrt{(2\pi)^1\sigma^2}}e^{-\frac{1}{2}(x-\mu)(\sigma^2)^{-1}(x-\mu)}\)</span>. However, our input is 3D, so we have to use 3D multivariate Gaussian <span class="math inline">\(\mathcal{N}_3(\mathbf{x}|\mathbf{\mu}, \Sigma)=\frac{1}{\sqrt{(2\pi)^3|\Sigma|}}e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^\top\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})}\)</span><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. In other words, we let <span class="math inline">\(p(\mathbf{x}_i|\mathbf{y}_i=\mathbf{k},\theta):=\mathcal{N}_3(\mathbf{x}_i|\mathbf{\mu}_\mathbf{k}, \Sigma_\mathbf{k})\)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;For more info on multivariate Gaussian and the covariance matrix <span class="math inline">\(\Sigma\)</span>, check <span class="citation" data-cites="prince2023understanding">Simon J. D. Prince (<a href="#ref-prince2023understanding" role="doc-biblioref">2023</a>, Appendix C.3.2)</span></p><div id="ref-prince2023understanding" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding Deep Learning</em>. The MIT Press. <a href="http://udlbook.com">http://udlbook.com</a>.
</div></div></div><p>Now let’s go check what’s the initial default distribution the model has started with.</p>
<div>
<div id="pyodide-4" class="exercise-cell">

</div>
<script type="pyodide-4-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiIyBwYXJhbWV0ZXJzXG50aGV0YSA9IFtcbiAgbnAuYXJyYXkoWzAuMCwgMC4wLCAwLjBdKSwgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICMgbXVfR09cbiAgbnAuZXllKDMpLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICMgc2lnbWFfR09cbiAgbnAuYXJyYXkoW05fZ3JlZW4gLyAoTl9ncmVlbitOX3JlZCtOX3llbGxvdyldKSwgICAgICAgICAgICMgcGhpX0dPXG4gIG5wLmFycmF5KFswLjAsIDAuMCwgMC4wXSksICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAjIG11X1NUT1BcbiAgbnAuZXllKDMpLCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICMgc2lnbWFfU1RPUFxuICBucC5hcnJheShbKE5fcmVkK05feWVsbG93KSAvIChOX2dyZWVuK05fcmVkK05feWVsbG93KV0pLCAgIyBwaGlfU1RPUFxuXVxuXG5zYW1wbGVzX2dvID0gbnAucmFuZG9tLm11bHRpdmFyaWF0ZV9ub3JtYWwodGhldGFbMF0sIHRoZXRhWzFdLCBOX2dyZWVuKVxuc2FtcGxlc19zdG9wID0gbnAucmFuZG9tLm11bHRpdmFyaWF0ZV9ub3JtYWwodGhldGFbM10sIHRoZXRhWzRdLCBOX3JlZCtOX3llbGxvdylcblxuZmlnLCBheCA9IHBsb3RfM2RfdHJhZmZpY19kYXRhKFxuICB4LCB5LCBcbiAgc2FtcGxlc19nbz1zYW1wbGVzX2dvLFxuICBzYW1wbGVzX3N0b3A9c2FtcGxlc19zdG9wXG4pXG5wbHQuc2hvdygpIn0=
</script>
</div>
<p>Basically, we want these distributions to match (so the model is able to classify correctly), which currently it is not.</p>
<p>Given <span class="math inline">\(\theta=\{\mu_\text{GO}, \Sigma_\text{GO}, \phi_\text{GO}, \mu_\text{STOP}, \Sigma_\text{STOP}, \phi_\text{STOP}\}\)</span><a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, we can finally define our MLE exactly as <span class="math display">\[
\begin{align}
\arg\max_\theta \frac{1}{n}\sum^n_{i=1} p(\mathbf{y}_i=\mathbf{k}|\mathbf{x}_i,\theta) &amp;= \arg\max_\theta  \frac{1}{n}\sum^n_{i=1} \frac{p(\mathbf{y}_i=\mathbf{k},\mathbf{x}_i|\theta)}{p(\mathbf{x}_i|\theta)} \\
     &amp;= \arg\max_\theta  \frac{1}{n}\sum^n_{i=1} \frac{p(\mathbf{x}_i|\mathbf{y}_i=\mathbf{k},\theta)p(\mathbf{y}_i=\mathbf{k}|\theta)}{\sum_{\mathbf{j}\in K} p(\mathbf{x}_i|\mathbf{y}_i=\mathbf{j},\theta)p(\mathbf{y}_i=\mathbf{j}|\theta)} \\
     &amp;= \arg\max_\theta  \frac{1}{n}\sum^n_{i=1} \frac{\mathcal{N}_3(\mathbf{x}_i|\mathbf{\mu}_\mathbf{k}, \Sigma_\mathbf{k}) p(\mathbf{y}_i=\mathbf{k}|\theta)}{\sum_{\mathbf{j}\in K} \mathcal{N}_3(\mathbf{x}_i|\mathbf{\mu}_\mathbf{j}, \Sigma_\mathbf{j})p(\mathbf{y}_i=\mathbf{j}|\theta)}
\end{align}
\]</span> <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;For each one-hot-encoded category <span class="math inline">\(\mathbf{k}\in K\)</span>, <span class="math inline">\(\mu_\mathbf{k}\)</span> and <span class="math inline">\(\Sigma_\mathbf{k}\)</span> represents the model’s parameter for each of the categories’ Normal distribution (mean and variance, respectively). <span class="math inline">\(\phi_\mathbf{k}\)</span> represents the ratio/weighting of each category <span class="math inline">\(\mathbf{k}\)</span>. So, <span class="math inline">\(p(\mathbf{y}_i=\mathbf{k}|\theta)=\phi_\mathbf{k}\)</span></p></div><div id="fn13"><p><sup>13</sup>&nbsp;<span class="math display">\[
\begin{align}
p(a,b|c) &amp;= \frac{p(a,b,c)}{p(c)} \\
  &amp;= \frac{p(a,b,c)p(b,c)}{p(c)p(b,c)} = \frac{p(a,b,c)}{p(b,c)}\frac{p(b,c)}{p(c)} \\
  &amp;= p(a|b,c)p(b|c)
\end{align}\]</span></p></div><div id="fn14"><p><sup>14</sup>&nbsp;The GDA/QDA, despite its name, actually has to go through a generative hoop for the derivation, since we are actually modelling our input <span class="math inline">\(\mathbf{x}_i\)</span> as some distribution which we can technically <em>re-sample from again</em> to <em>generate</em> a new sample of the similiar distribution to <span class="math inline">\(\mathbf{x}_i\)</span>.</p></div></div><p>In fact, learning a model where we assume the <span class="math inline">\(\mathbf{x}\)</span>’s in each category of the categorical distribution <span class="math inline">\(\mathbf{y}\)</span> as a Gaussian is known as Gaussian Discriminative Analysis (GDA) or Quadratic Discriminative Analysis (QDA) which can you read more <a href="https://kuleshov-group.github.io/aml-book/contents/lecture7-gaussian-discriminant-analysis.html#gaussian-discriminant-analysis">here</a>. Additionally, this is very close to the most well-known model/learner out there: Naïve Bayes. Naïve Bayes assumes each input data point <span class="math inline">\(\mathbf{x}_i\)</span> are independent of each other, but GDA/QDA assumes each input data point <span class="math inline">\(\mathbf{x}_i\)</span> follow a Gaussian distribution as mentioned before. <a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<section id="analytical-estimation-of-theta" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="analytical-estimation-of-theta">Analytical Estimation of <span class="math inline">\(\theta\)</span></h2>
<p>Since GDA/QDA are clean to work with, they have an analytical method to compute the optimal parameter <span class="math inline">\(\theta\)</span> given the data <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. We basically compute the derivative of the log-likelihood with respect to each parameters to obtain the following <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;Check the derivations <a href="https://kuleshov-group.github.io/aml-book/contents/lecture7-gaussian-discriminant-analysis.html#optimizing-the-log-likelihood">here</a></p></div></div><p>We find that <span class="math inline">\(\phi_k=\frac{n_\mathbf{k}}{n}\)</span> where <span class="math inline">\(n\)</span> is the total number of samples/data while <span class="math inline">\(n_\mathbf{k}\)</span> is the number of samples with <span class="math inline">\(\mathbf{y}_i=\mathbf{k}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\mathbf{\mu}_\mathbf{k} &amp;= \frac{\sum_{i \text{  s.t.  } \mathbf{y}_i=\mathbf{k}} \mathbf{x}_i}{n_\mathbf{k}} \\
\Sigma_\mathbf{k} &amp;= \frac{\sum_{i \text{  s.t.  } \mathbf{y}_i=\mathbf{k}} (\mathbf{x}_i-\mathbf{\mu}_\mathbf{k})(\mathbf{x}_i-\mathbf{\mu}_\mathbf{k})^\top}{n_\mathbf{k}} \\
\end{align}
\]</span></p>
<div>
<div id="pyodide-5" class="exercise-cell">

</div>
<script type="pyodide-5-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZ29fbWFza19pZHggPSB5WzosIDBdID09IDFcbnN0b3BfbWFza19pZHggPSB5WzosIDFdID09IDFcblxueF9nbyA9IHhbZ29fbWFza19pZHhdXG54X3N0b3AgPSB4W3N0b3BfbWFza19pZHhdXG5uX2dvID0geF9nby5zaGFwZVswXVxubl9zdG9wID0geF9zdG9wLnNoYXBlWzBdXG5cbm11X2dvID0gbnAuc3VtKHhfZ28sIGF4aXM9MCkgLyBuX2dvXG5tdV9zdG9wID0gbnAuc3VtKHhfc3RvcCwgYXhpcz0wKSAvIG5fc3RvcFxuXG5kaWZmX2dvID0geF9nbyAtIG11X2dvXG5kaWZmX3N0b3AgPSB4X3N0b3AgLSBtdV9zdG9wXG4jIHRoZSB0cmFuc3Bvc2UgYXJlIGZsaXBwZWQgc2luY2Ugd2UgYXJlIHVzaW5nIHJvdyB2ZWN0b3JzXG4jIGhlcmUgKGNvbW1vbiBpbiBjb2RlKSBpbnN0ZWFkIG9mIGNvbC4gdmVjdG9yc1xuIyAoY29tbW9uIGluIHRleHQgZm9ybSlcbnNpZ21hX2dvID0gKGRpZmZfZ28uVCBAIGRpZmZfZ28pIC8gbl9nb1xuc2lnbWFfc3RvcCA9IChkaWZmX3N0b3AuVCBAIGRpZmZfc3RvcCkgLyBuX3N0b3BcblxuIyB1c2VkIG9ubHkgd2hlbiBjb21wdXRpbmcgdGhlIHByZWRpY3Rpb25cbiMgKGkuZS4sIHRoZSBmaW5hbCBjbGFzc2lmaWNhdGlvbiBwcm9iYWJpbGl0eSBvZiB0aGUgaW1hZ2VzKVxucGhpX2dvID0gbl9nbyAvIChuX2dvICsgbl9zdG9wKVxucGhpX3N0b3AgPSBuX3N0b3AgLyAobl9nbyArIG5fc3RvcClcblxudGhldGEgPSBbbXVfZ28sIHNpZ21hX2dvLCBwaGlfZ28sIG11X3N0b3AsIHNpZ21hX3N0b3AsIHBoaV9zdG9wXVxuXG5zYW1wbGVzX2dvID0gbnAucmFuZG9tLm11bHRpdmFyaWF0ZV9ub3JtYWwoXG4gIHRoZXRhWzBdLCB0aGV0YVsxXSwgTl9ncmVlblxuKVxuc2FtcGxlc19zdG9wID0gbnAucmFuZG9tLm11bHRpdmFyaWF0ZV9ub3JtYWwoXG4gIHRoZXRhWzNdLCB0aGV0YVs0XSwgTl9yZWQrTl95ZWxsb3dcbilcblxuZmlnLCBheCA9IHBsb3RfM2RfdHJhZmZpY19kYXRhKFxuICB4LCB5LFxuICBzYW1wbGVzX2dvPXNhbXBsZXNfZ28sXG4gIHNhbXBsZXNfc3RvcD1zYW1wbGVzX3N0b3AsXG4pXG5wbHQuc2hvdygpIn0=
</script>
</div>
</section>
<section id="gradient-descent-estimation-of-theta" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-estimation-of-theta">Gradient Descent Estimation of <span class="math inline">\(\theta\)</span></h2>
<p>Since we are using QDAs/GDAs, we have analytical solution available us (and we should use it in practice if QDAs/GDAs is what we really want). But most ML models, especially in vision, uses more complicated formulations like neural networks, we have to resort using numerical optimizations like gradient descent (GD), stochastic gradient descent, and Adam. So let us see how we optimize model in the majority of the cases via GD over the negative log-likelihood (NLL), which is just the negation of the log of the likelihood <span class="math inline">\(\frac{1}{n}\sum^n_{i=1} p(\mathbf{y}_i=\mathbf{k}|\mathbf{x}_i,\theta)\)</span>, including the NLL of the Gaussian in <code>def neg_log_likelihood_gaussian</code>. Note, we omit <span class="math inline">\(\log\phi_\mathbf{k}\)</span> since we are not optimizing that.</p>
<p>This will take about a few minutes. Make sure you run the previous cell.</p>
<div>
<div id="pyodide-6" class="exercise-cell">

</div>
<script type="pyodide-6-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImZpZy1hbGlnbiI6ImNlbnRlciJ9LCJjb2RlIjoiXG5pbXBvcnQgYXV0b2dyYWQubnVtcHkgYXMgYW5wXG5mcm9tIGF1dG9ncmFkIGltcG9ydCBncmFkXG5pbXBvcnQgbnVtcHkgYXMgbnBcbmZyb20gbWF0cGxvdGxpYiBpbXBvcnQgcHlwbG90IGFzIHBsdFxuZnJvbSBtYXRwbG90bGliLmFuaW1hdGlvbiBpbXBvcnQgRnVuY0FuaW1hdGlvblxuZnJvbSBJUHl0aG9uLmRpc3BsYXkgaW1wb3J0IEhUTUxcblxuZGVmIF9zeW1tZXRyaXplKE0pOlxuICAgIHJldHVybiAwLjUgKiAoTSArIE0uVClcblxuZGVmIF9tYWtlX3NwZChNLCBlcHM9MWUtNSk6XG4gICAgTSA9IF9zeW1tZXRyaXplKE0pXG4gICAgdywgViA9IG5wLmxpbmFsZy5laWdoKE0pICAjIE9LIHRvIHVzZSBudW1weSBoZXJlOyB3ZSBkb24ndCBiYWNrcHJvcCB0aHJvdWdoIHRoaXMgcHJvamVjdGlvblxuICAgIHcgPSBucC5tYXhpbXVtKHcsIGVwcylcbiAgICByZXR1cm4gKFYgKiB3KSBAIFYuVFxuXG4jIHBlci1jbGFzcyBuZWdhdGl2ZSBsb2ctbGlrZWxpaG9vZCBmb3IgYSBmdWxsIEdhdXNzaWFuXG5kZWYgbmVnX2xvZ19saWtlbGlob29kX2dhdXNzaWFuKG11LCBzaWdtYSwgeF9jbGFzcyk6XG4gICAgbiwgZCA9IHhfY2xhc3Muc2hhcGVcblxuICAgICMgb3B0aW9uYWwgcmVndWxhcml6ZXIgZm9yIHN0YWJpbGl0eSAoYnV0IHRoYXQgbWFrZXMgTUxFIGludG8gYSBNQVApXG4gICAgc2lnbWFfcmVnID0gc2lnbWEgIyArIDFlLTQgKiBhbnAuZXllKGQpXG4gICAgc2lnbiwgbG9nZGV0ID0gYW5wLmxpbmFsZy5zbG9nZGV0KHNpZ21hX3JlZykgICAgIyBzaG91bGQgYmUgKzEgZm9yIFNQRFxuICAgIGludl9zaWdtYSA9IGFucC5saW5hbGcuaW52KHNpZ21hX3JlZylcblxuICAgIGRpZmYgPSB4X2NsYXNzIC0gbXVcbiAgICAjIHF1YWRyYXRpYyB0ZXJtIHBlciBzYW1wbGU6ICh4Lc68KV5UIM6jXnstMX0gKHgtzrwpXG4gICAgcXVhZCA9IGFucC5zdW0oZGlmZiAqIChkaWZmIEAgaW52X3NpZ21hKSwgYXhpcz0xKVxuXG4gICAgIyBhdmVyYWdlIE5MTFxuICAgIHJldHVybiAwLjUgKiAoZCAqIGFucC5sb2coMiphbnAucGkpICsgbG9nZGV0KSArIDAuNSAqIGFucC5tZWFuKHF1YWQpXG5cbnhfZ29fZGF0YSAgID0geFtnb19tYXNrXVxueF9zdG9wX2RhdGEgPSB4W3N0b3BfbWFza11cblxuIyBpbml0aWFsaXplIHdpdGggemVybyBtZWFuIGFuZCBpZGVudGl0eSBjb3ZhcmlhbmNlcyBtYXRyaXggKHZhcmlhbmNlPTEpXG5ucC5yYW5kb20uc2VlZCg0Milcbm11X2dvX2dkICAgPSBucC56ZXJvcyh4LnNoYXBlWzFdKVxubXVfc3RvcF9nZCA9IG5wLnplcm9zKHguc2hhcGVbMV0pXG5zaWdtYV9nb19nZCAgID0gbnAuZXllKHguc2hhcGVbMV0pXG5zaWdtYV9zdG9wX2dkID0gbnAuZXllKHguc2hhcGVbMV0pXG5cbiMgY29tcHV0ZSB0aGUgZ3JhZGllbnQgb2YgdGhlIGZ1bmN0aW9uIHcuci50LiBwYXJhbWV0ZXIgaW5kZXhcbmdyYWRfbXUgICAgPSBncmFkKG5lZ19sb2dfbGlrZWxpaG9vZF9nYXVzc2lhbiwgMClcbmdyYWRfc2lnbWEgPSBncmFkKG5lZ19sb2dfbGlrZWxpaG9vZF9nYXVzc2lhbiwgMSlcblxuIyBvcHRpbWl6YXRpb25cbmFuaW1hdGlvbl9zbmFwc2hvdHMgPSBbXVxubHJfbXUgPSAwLjFcbmxyX3NpZ21hID0gMC4wNVxubnVtX2l0ZXJhdGlvbnMgPSAxMDBcbmNsaXAgPSAxMC4wXG5cbmZvciBpdCBpbiByYW5nZShudW1faXRlcmF0aW9ucyArIDEpOlxuICAgICMgY292YXJpYW5jZSBtYXRyaXggdmVyeSBzZW5zaXRpdmUgKG5lZWQgdG8gZW1wbG95IHNjaGVkdWxlcilcbiAgICBpZiBpdCA9PSAyNTpcbiAgICAgIGxyX3NpZ21hID0gMC4wNVxuICAgIGVsaWYgaXQgPT0gNTA6XG4gICAgICBscl9zaWdtYSA9IDAuMDFcbiAgICAgIGxyX211ID0gMC4wMVxuICAgIGVsaWYgaXQgPT0gNzU6XG4gICAgICBscl9zaWdtYSA9IDAuMDAxXG4gIFxuICAgIGlmIGl0ICUgNSA9PSAwOlxuICAgICAgICAjIGNvbXB1dGUgbG9zcyBhcyBpc1xuICAgICAgICBsb3NzX2dvICAgPSBuZWdfbG9nX2xpa2VsaWhvb2RfZ2F1c3NpYW4obXVfZ29fZ2QsICAgc2lnbWFfZ29fZ2QsICAgeF9nb19kYXRhKVxuICAgICAgICBsb3NzX3N0b3AgPSBuZWdfbG9nX2xpa2VsaWhvb2RfZ2F1c3NpYW4obXVfc3RvcF9nZCwgc2lnbWFfc3RvcF9nZCwgeF9zdG9wX2RhdGEpXG4gICAgICAgIHRvdGFsID0gZmxvYXQobG9zc19nbyArIGxvc3Nfc3RvcClcblxuICAgICAgICBhbmltYXRpb25fc25hcHNob3RzLmFwcGVuZCh7XG4gICAgICAgICAgICAnaXRlcic6IGl0LFxuICAgICAgICAgICAgJ211X2dvJzogICBtdV9nb19nZC5jb3B5KCksXG4gICAgICAgICAgICAnbXVfc3RvcCc6IG11X3N0b3BfZ2QuY29weSgpLFxuICAgICAgICAgICAgJ3NpZ21hX2dvJzogICBzaWdtYV9nb19nZC5jb3B5KCksXG4gICAgICAgICAgICAnc2lnbWFfc3RvcCc6IHNpZ21hX3N0b3BfZ2QuY29weSgpLFxuICAgICAgICAgICAgJ2xvc3MnOiB0b3RhbFxuICAgICAgICB9KVxuICAgICAgICBpZiBpdCAlIDIwID09IDA6XG4gICAgICAgICAgICBwcmludChmXCJJdGVyIHtpdDozZH06IExvc3M9e3RvdGFsOi40Zn0gfCDOvF9HTz17bXVfZ29fZ2Qucm91bmQoMyl9IHwgzrxfU1RPUD17bXVfc3RvcF9nZC5yb3VuZCgzKX1cIilcblxuICAgIGlmIGl0ID09IG51bV9pdGVyYXRpb25zOlxuICAgICAgICBicmVha1xuXG4gICAgIyB1c2UgdGhlIGNvbXB1dGVkIGdyYWRpZW50IGZ1bmN0aW9uXG4gICAgZ19tdV9nbyAgICA9IGdyYWRfbXUobXVfZ29fZ2QsIHNpZ21hX2dvX2dkLCB4X2dvX2RhdGEpXG4gICAgZ19zaWdtYV9nbyA9IGdyYWRfc2lnbWEobXVfZ29fZ2QsIHNpZ21hX2dvX2dkLCB4X2dvX2RhdGEpXG4gICAgZ19tdV9zdG9wICAgID0gZ3JhZF9tdShtdV9zdG9wX2dkLCBzaWdtYV9zdG9wX2dkLCB4X3N0b3BfZGF0YSlcbiAgICBnX3NpZ21hX3N0b3AgPSBncmFkX3NpZ21hKG11X3N0b3BfZ2QsIHNpZ21hX3N0b3BfZ2QsIHhfc3RvcF9kYXRhKVxuXG4gICAgIyBncmFkaWVudCBjbGlwcGluZyBmb3Igc3RhYmlsaXR5XG4gICAgZm9yIGcgaW4gKGdfbXVfZ28sIGdfc2lnbWFfZ28sIGdfbXVfc3RvcCwgZ19zaWdtYV9zdG9wKTpcbiAgICAgICAgZ24gPSBucC5saW5hbGcubm9ybShucC5yYXZlbChucC5hcnJheShnKSkpXG4gICAgICAgIGlmIG5wLmlzZmluaXRlKGduKSBhbmQgZ24gPiBjbGlwOlxuICAgICAgICAgICAgZyAqPSAoY2xpcCAvIGduKVxuXG4gICAgIyBncmFkaWVudCB1cGRhdGVcbiAgICBtdV9nb19nZCAgID0gbXVfZ29fZ2QgICAtIGxyX211KmdfbXVfZ29cbiAgICBtdV9zdG9wX2dkID0gbXVfc3RvcF9nZCAtIGxyX211KmdfbXVfc3RvcFxuICAgIHNpZ21hX2dvX2dkICAgPSBzaWdtYV9nb19nZCAgIC0gbHJfc2lnbWEqZ19zaWdtYV9nb1xuICAgIHNpZ21hX3N0b3BfZ2QgPSBzaWdtYV9zdG9wX2dkIC0gbHJfc2lnbWEqZ19zaWdtYV9zdG9wXG5cbiAgICAjIGtlZXAgzqMgc3ltbWV0cmljICsgcG9zaXRpdmUgZGVmaW5pdGVcbiAgICBzaWdtYV9nb19nZCAgID0gX21ha2Vfc3BkKHNpZ21hX2dvX2dkKVxuICAgIHNpZ21hX3N0b3BfZ2QgPSBfbWFrZV9zcGQoc2lnbWFfc3RvcF9nZClcblxucHJpbnQoXCJcXG5GaW5hbCBSZXN1bHRzOlwiKVxucHJpbnQoXCJcXHRHcmFkaWVudCBEZXNjZW50OlwiKVxucHJpbnQoZlwiXFx0XFx0zrxfR086ICAgICAgICAgICAgICAgICAgIHttdV9nb19nZC5yb3VuZCgzKX1cIilcbnByaW50KGZcIlxcdFxcdM68X1NUT1A6ICAgICAgICAgICAgICAgICB7bXVfc3RvcF9nZC5yb3VuZCgzKX1cIilcbnByaW50KGZcIlxcdFxcdM6jX0dPIGRpYWcgKGkuZS4sIM+DwrIpOiAgIHtucC5kaWFnKHNpZ21hX2dvX2dkKS5yb3VuZCgzKX1cIilcbnByaW50KGZcIlxcdFxcdM6jX1NUT1AgZGlhZyAoaS5lLiwgz4PCsik6IHtucC5kaWFnKHNpZ21hX3N0b3BfZ2QpLnJvdW5kKDMpfVwiKVxucHJpbnQoXCJcXHRBbmFseXRpY2FsOlwiKVxucHJpbnQoZlwiXFx0XFx0zrxfR086ICAgICAgICAgICAgICAgICAgIHttdV9nby5yb3VuZCgzKX1cIilcbnByaW50KGZcIlxcdFxcdM68X1NUT1A6ICAgICAgICAgICAgICAgICB7bXVfc3RvcC5yb3VuZCgzKX1cIilcbnByaW50KGZcIlxcdFxcdM6jX0dPIGRpYWcgKGkuZS4sIM+DwrIpOiAgIHtucC5kaWFnKHNpZ21hX2dvKS5yb3VuZCgzKX1cIilcbnByaW50KGZcIlxcdFxcdM6jX1NUT1AgZGlhZyAoaS5lLiwgz4PCsik6IHtucC5kaWFnKHNpZ21hX3N0b3ApLnJvdW5kKDMpfVwiKVxuXG5maWcgPSBwbHQuZmlndXJlKGZpZ3NpemU9KDgsIDcpKVxuYXggPSBmaWcuYWRkX3N1YnBsb3QoMTExLCBwcm9qZWN0aW9uPSczZCcpXG5cbmRlZiB1cGRhdGUoZnJhbWUpOlxuICAgIGF4LmNsZWFyKClcbiAgICBzbmFwID0gYW5pbWF0aW9uX3NuYXBzaG90c1tmcmFtZV1cblxuICAgICMgR2VuZXJhdGUgY29uc2lzdGVudCBzYW1wbGVzIGZvciB0aGlzIGZyYW1lXG4gICAgcm5nID0gbnAucmFuZG9tLmRlZmF1bHRfcm5nKGZyYW1lKVxuICAgIG5fc2FtcGxlcyA9IDUwXG4gICAgZ29fcyAgID0gcm5nLm11bHRpdmFyaWF0ZV9ub3JtYWwoc25hcFsnbXVfZ28nXSwgICBzbmFwWydzaWdtYV9nbyddLCAgIG5fc2FtcGxlcylcbiAgICBzdG9wX3MgPSBybmcubXVsdGl2YXJpYXRlX25vcm1hbChzbmFwWydtdV9zdG9wJ10sIHNuYXBbJ3NpZ21hX3N0b3AnXSwgbl9zYW1wbGVzKVxuXG4gICAgIyBVc2UgbW9kdWxhciBwbG90dGluZyBmdW5jdGlvbiB3aXRoIGV4aXN0aW5nIGF4aXNcbiAgICBwbG90XzNkX3RyYWZmaWNfZGF0YShcbiAgICAgIHgsIHksIHNhbXBsZXNfZ289Z29fcywgc2FtcGxlc19zdG9wPXN0b3BfcywgYXg9YXgsXG4gICAgICB0aXRsZT1mJ0l0ZXJhdGlvbiB7c25hcFtcIml0ZXJcIl19IHwgTG9zczoge3NuYXBbXCJsb3NzXCJdOi40Zn0nXG4gICAgKVxuXG5hbmltID0gRnVuY0FuaW1hdGlvbihmaWcsIHVwZGF0ZSwgZnJhbWVzPWxlbihhbmltYXRpb25fc25hcHNob3RzKSwgaW50ZXJ2YWw9MzAwLCByZXBlYXQ9VHJ1ZSlcbnBsdC5jbG9zZSgpXG5IVE1MKGFuaW0udG9fanNodG1sKCkpIn0=
</script>
</div>
<p>You can see how the model distribution gets closer to the true data distribution for each gradient step. Remember, each point is a data point representing the 3-pixel image of a traffic light. Theoretically speaking, if there exists an analytical solution of a quadratic expression (single minimum), gradient descent should effectively reach to the same optimal point. However, the reason it’s not is likely because we’re missing the class prior <span class="math inline">\(\phi_\mathbf{k}\)</span> or the variance (i.e., the spread) is different between the two categories (needing two different learning rate?) Regardless, it still mostly converges.</p>
</section>
</section>
<section id="summary" class="level1 page-columns page-full">
<h1>Summary</h1>
<p>Overall, you saw a simple GDA model to classify a 3-pixel image input of a traffic light as either <span class="math inline">\(\text{GO}\)</span> or <span class="math inline">\(\text{STOP}\)</span>, which would be used to tell the car to move or not. But if you recall, this is a simple abstraction. In a more realistic setting, we would have our models read from real images (i.e., an actual image of a traffic light), so you can expect the distribution would not just be over 3D (i.e., the three pixels), but one million dimensions! We also get to see how the model themselves is actually just a small part of the greater learning system. Instead of a normal distribution <span class="math inline">\(\mathcal{N}\)</span>, we can replace it with a multi-layered neural networks, or even augment it with convolutional layers, and the main idea still doesn’t change.</p>
<p>So, going back to the original three questions:</p>
<ul>
<li>What are they?</li>
</ul>
<p>MAPs and MLEs are particular learning schemes we can use on our models to estimate an optimal <span class="math inline">\(\theta\)</span> given <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. It also has a more general version where <span class="math inline">\(\theta\)</span> is modelled in a Bayesian/probabilistic way (i.e., Bayesian prediction).</p>
<ul>
<li>How do they relate to vision?</li>
</ul>
<p>Because probabilistic learners are not just restricted to tabular data, allowing other forms of data to train on the model (i.e., images). From a classical vision standpoint, assuming the model and the learning process as probabilistic allows the use of various powerful, abstract tools from statistics to model arbitrary distributions of visual data (i.e., quadratic discriminative analysis), whereas one needs to design different models for different domains. Refer <span class="citation" data-cites="sutton2019bitter">Sutton (<a href="#ref-sutton2019bitter" role="doc-biblioref">2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><ul>
<li>Why are they useful to think about them?</li>
</ul>
<p>Nature is too complicated. Probability is <del>likely</del> the best way to effectively model complex processes into something simpler <em>to work with</em>, allowing effective training of the model for seemingly intelligent prediction. Refer <span class="citation" data-cites="sutton2019bitter">Sutton (<a href="#ref-sutton2019bitter" role="doc-biblioref">2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sutton2019bitter" class="csl-entry" role="listitem">
Sutton, Richard S. 2019. <span>“The Bitter Lesson.”</span> <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" class="uri">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>.
</div></div><p>More practically, all deep learning-based vision models employs these statistical tools, and even moreso on modern generative models. So, to learn and apply these latest models, one likely needs to have strong foundation in these aspects.</p>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yOC4xL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIl19fQ==
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-6","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_6 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-6-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_6 = pyodideOjs.process(_pyodide_editor_6, {});\n"},{"cellName":"pyodide-5","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_5 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-5-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_5 = pyodideOjs.process(_pyodide_editor_5, {});\n"},{"cellName":"pyodide-4","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_4 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-4-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_4 = pyodideOjs.process(_pyodide_editor_4, {});\n"},{"cellName":"pyodide-3","inline":false,"methodName":"interpret","source":"_pyodide_value_3 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-3-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"3\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"3\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"_pyodide_value_1 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-1-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"1\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"1\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
<script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>

</section>


</main> <!-- /main -->
<script type="ojs-module-contents">
eyJjb250ZW50cyI6W119
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../blog";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>