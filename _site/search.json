[
  {
    "objectID": "blog_entry.html",
    "href": "blog_entry.html",
    "title": "",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n(WIP) Toy VGGT\n\n\nExploring the Latest in Vision\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Learning\n\n\nA Review on Computer Vision Fundamentals\n\n\n\nNov 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n(WIP)\n\n\nPath-Space Differentiable Rendering\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\nInteresting Topics For November\n\n\nGeneral\n\n\n\nInvalid Date\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/10_uavf_perception_wk7.html",
    "href": "blog/10_uavf_perception_wk7.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/10 copy.html",
    "href": "blog/10 copy.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/10 copy 5.html",
    "href": "blog/10 copy 5.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/10 copy 3.html",
    "href": "blog/10 copy 3.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/10 copy 2.html",
    "href": "blog/10 copy 2.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/10 copy 4.html",
    "href": "blog/10 copy 4.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/10 copy 6.html",
    "href": "blog/10 copy 6.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/10.html",
    "href": "blog/10.html",
    "title": "Probabilities & Convolutions",
    "section": "",
    "text": "Introduction\nUnfortunately, there‚Äôs going to be a lot of text this time despite the many interactive codes present. So, this blog will be divided into two parts: first, a general, interactive introduction to the concepts we will be reviewing; second, an optional, more in-depth discussion of said topics.\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte‚Äôs styles in RMarkdown documents (Sutton 2019). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nSutton, Richard S. 2019. ‚ÄúThe Bitter Lesson.‚Äù http://www.incompleteideas.net/IncIdeas/BitterLesson.html.\n1¬†To learn more, you can read more about Quarto or visit Quarto‚Äôs Github repository.\n\n\n\n\n\n\n\nOverview: MAP & MLE\n\n\nOverview: Convolution\n\n\nIn-depth: MAP & MLE\n\n\nIn-depth: Convolution"
  },
  {
    "objectID": "blog/20.html",
    "href": "blog/20.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "You have probably seen this thing floating around in several machine learning courses: \\(P(y|x,\\theta)\\), \\(P(\\theta|x,y)\\), or even the term MAP and MLE. If you haven‚Äôt yet, you will probably start seeing them after. These concepts comes from statistics where they‚Äôre called statistical learning, parameter estimation, probabilistic inference, or one of their many synonyms. But what are they? How do they relate to vision? And why is it useful to think about them?\nLet us start with a toy example. We want to design an autonomous system to tell when the car should go or stop, depending on the traffic light. Our dataset are pictures of traffic lights and we want our model \\(M(\\theta)\\) to classify whether an image is \\(\\text{GO}\\) or \\(\\text{STOP}\\) parameterized by \\(\\theta\\). Using one-hot encoding, we let \\(\\mathbf{y}_i:=\\begin{bmatrix} 1 & 0 \\end{bmatrix}^\\top\\) for \\(\\text{GO}\\) and \\(\\mathbf{y}_i:=\\begin{bmatrix} 0 & 1 \\end{bmatrix}^\\top\\) for \\(\\text{STOP}\\). Let red and yellow light be \\(\\text{STOP}\\) and green light be \\(\\text{GO}\\) for our intents and purposes.\n\n\n\n1\n1¬†Image copied from here\n\nTo simplify our case even more, let us abstract away the high-dimensional input of an image and let \\(\\mathbf{x}_i:=\\begin{bmatrix} x_r & x_y & x_g \\end{bmatrix}^\\top\\), where each component \\(\\in[0,1]\\) corresponds to a snapshot of the illumination intensity of red \\(x_r\\), yellow \\(x_y\\), and the green \\(x_g\\) bulb of a traffic light. So, we can visualize our data as‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have introduced our scenario, let us get into the main idea of this blog."
  },
  {
    "objectID": "blog/20.html#maximum-likeilhood-estimation-mle",
    "href": "blog/20.html#maximum-likeilhood-estimation-mle",
    "title": "Statistical Learning",
    "section": "Maximum Likeilhood Estimation (MLE)",
    "text": "Maximum Likeilhood Estimation (MLE)\n\\(\\hat{\\theta}=\\arg\\max_{\\theta}p(\\mathbf{y}|\\mathbf{x},\\theta)\\)\nRemember, \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) are our training data and cannot be changed. The only thing we can (and should) change is the model parameter \\(\\theta\\). So, this is basically saying what value of \\(\\theta\\) can maximize the probability of the correct label \\(\\mathbf{y}\\) from its corresponding input \\(\\mathbf{x}\\) for all samples (i.e., data points). The \\(p(\\mathbf{y}|\\mathbf{x},\\theta)\\) is known as the likelihood of \\(\\theta\\). We will define what \\(\\theta\\) is later, but basically it is an abstract representation of model‚Äôs parameter, representing choices of hypothesis that best explains data relationship."
  },
  {
    "objectID": "blog/20.html#maximum-a-posteriori-map-estimation",
    "href": "blog/20.html#maximum-a-posteriori-map-estimation",
    "title": "Statistical Learning",
    "section": "Maximum a Posteriori (MAP) Estimation",
    "text": "Maximum a Posteriori (MAP) Estimation\n\\(\\hat{\\theta}=\\arg\\max_{\\theta}p(\\theta|\\mathbf{x},\\mathbf{y})=\\arg\\max_{\\theta}\\frac{p(\\mathbf{y}|\\mathbf{x},\\theta)p(\\theta)}{p(\\mathbf{x},\\mathbf{y})}\\propto \\arg\\max_{\\theta}p(\\mathbf{y}|\\mathbf{x},\\theta)p(\\theta)\\)\nThis is saying which parameter \\(\\theta\\) for our model (i.e., weights) has the highest probability that explains the data relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). In other words, this maximizes the posterior distribution \\(P(\\theta|\\mathbf{x},\\mathbf{y})\\) (i.e., the posterior of \\(\\theta\\)). We then use the Bayes‚Äô theorem 5 to get back our likelihood again with an additional prior \\(p(\\theta)\\) 6 with a constant factor \\(\\frac{1}{p(\\mathbf{x},\\mathbf{y})}\\) which can be ignored 7.\n5¬†A commonly used special operation in statistics/probabilities. Check here6¬†There‚Äôs many ways to think about prior \\(p(\\theta)\\). You think of it as a weighting factor of the likelihood that says how likely that likelihood is given the selected \\(\\theta\\). Or, as a balance between data-driven likelihood and previously known (i.e., ‚Äúprior‚Äù) knowledge on how \\(\\theta\\) should behave (e.g., \\(\\theta\\) is likely to be 67 for some reason). Or, as a regularizer where we follow Occam‚Äôs razor that the weights should be simple.7¬†This is a probability over the training data \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\). It‚Äôs not going to change throughout the maximization process of \\(\\theta\\)."
  },
  {
    "objectID": "blog/20.html#bayesian-inference",
    "href": "blog/20.html#bayesian-inference",
    "title": "Statistical Learning",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\\(p(y^*|x^*,\\mathbf{x},\\mathbf{y})=\\int p(y^*|x^*,\\theta)p(\\theta|\\mathbf{x},\\mathbf{y})d\\theta\\)\nInstead of estimating a specific parameter \\(\\theta\\) (i.e., point estimate), we interpret the model‚Äôs parameters \\(\\theta\\) as probabilistic 8. Notice we are not maximizing anything or even calculating \\(\\theta\\) itself, but directly predicting our unseen data point \\(y^*\\) given \\(x^*\\). In fact, this is a summation of each prediction \\(p(y^*|x^*,\\theta)\\) from all possible parameter \\(\\theta\\) weighted by the posterior for each \\(\\theta\\). The integral can sometimes be computed analytically given a good conjugate prior 9, but most of the time, it is approximated by sampling (i.e., Monte-Carlo).\n8¬†In statistics, there are two interpretations: frequentist (there‚Äôs a single true value and the randomness is strictly from sampling error like MLE/MAP) and Bayesian (there are no true value and the probability is intrinsic).9¬†A special prior when combined with its corresponding likelihood produces a nice-to-work-with analytical posterior (remember, we are integrating the posterior, which is nasty most of the time). For example, the Gaussian distribution \\(\\mathcal{N}(\\mu,\\sigma^2)\\)‚Äôs conjugate prior is Normal-inverse gamma. Check here."
  },
  {
    "objectID": "blog/20.html#others",
    "href": "blog/20.html#others",
    "title": "Statistical Learning",
    "section": "Others",
    "text": "Others\nFor generative or unsupervised models, we don‚Äôt necessarily have a label \\(\\mathbf{y}\\). What we do instead is have the model learn the data distribution of the input \\(\\mathbf{x}\\) itself, resulting in posteriors with \\(p(\\theta|\\mathbf{x})\\) instead of \\(p(\\theta|\\mathbf{x},\\mathbf{y})\\), or likelihood of \\(p(\\mathbf{x}|\\theta)\\) instead of \\(p(\\mathbf{y}|\\mathbf{x},\\theta)\\). Sometimes, they also have the latent distribution \\(\\mathbf{z}\\) as a more efficient, intermediate way to have the model learn the distribution. For example, VAEs Simon J. D. Prince (2023, chap. 17.3) and diffusions Simon J. D. Prince (2023, chap. 18.4) are learned via MLE with \\(p(\\mathbf{x}|\\theta)\\) as the likelihood. In some cases, we do a maximization and a minimization of two different sets of parameters \\(\\theta\\) and \\(\\phi\\) like in GANs Simon J. D. Prince (2023, chap. 15.1.1). Or, in Deep RL, we maximize the policy Simon J. D. Prince (2023, chap. 19.3).\nFor the purpose of learning (i.e., you learning this), we will stick with much more simpler MLE under supervised discriminative learning. Now enough with the theory and see some visuals from our example scenario."
  },
  {
    "objectID": "blog/20.html#analytically-estimate-theta",
    "href": "blog/20.html#analytically-estimate-theta",
    "title": "(WIP) Statistical Learning",
    "section": "Analytically Estimate \\(\\theta\\)",
    "text": "Analytically Estimate \\(\\theta\\)\nSince GDA/QDA are clean to work with, they have an analytical method to compute the optimal parameter \\(\\theta\\) given the data \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). We basically compute the derivative of the log-likelihood with respect to each parameters to obtain the following 15.\n15¬†Check the derivations hereWe find that \\(\\phi_k=\\frac{n_\\mathbf{k}}{n}\\) where \\(n\\) is the total number of samples/data while \\(n_\\mathbf{k}\\) is the number of samples with \\(\\mathbf{y}_i=\\mathbf{k}\\).\n\\[\n\\begin{align}\n\\mathbf{\\mu}_\\mathbf{k} &= \\frac{\\sum_{i \\text{  s.t.  } \\mathbf{y}_i=\\mathbf{k}} \\mathbf{x}_i}{n_\\mathbf{k}} \\\\\n\\Sigma_\\mathbf{k} &= \\frac{\\sum_{i \\text{  s.t.  } \\mathbf{y}_i=\\mathbf{k}} (\\mathbf{x}_i-\\mathbf{\\mu}_\\mathbf{k})(\\mathbf{x}_i-\\mathbf{\\mu}_\\mathbf{k})^\\top}{n_\\mathbf{k}} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/20.html#gradient-descent-on-theta",
    "href": "blog/20.html#gradient-descent-on-theta",
    "title": "(WIP) Statistical Learning",
    "section": "Gradient Descent on \\(\\theta\\)",
    "text": "Gradient Descent on \\(\\theta\\)\nWhile the analytical solution is elegant, most ML problems don‚Äôt have closed-form solutions. Let‚Äôs see how gradient descent iteratively finds the parameters by maximizing the log-likelihood.\n\n\n\n\n\n\nYou can see how the model distribution gets closer to the true data distribution for each gradient step."
  },
  {
    "objectID": "blog/20.html#analytical-estimation-of-theta",
    "href": "blog/20.html#analytical-estimation-of-theta",
    "title": "Statistical Learning",
    "section": "Analytical Estimation of \\(\\theta\\)",
    "text": "Analytical Estimation of \\(\\theta\\)\nSince GDA/QDA are clean to work with, they have an analytical method to compute the optimal parameter \\(\\theta\\) given the data \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). We basically compute the derivative of the log-likelihood with respect to each parameters to obtain the following 15.\n15¬†Check the derivations hereWe find that \\(\\phi_k=\\frac{n_\\mathbf{k}}{n}\\) where \\(n\\) is the total number of samples/data while \\(n_\\mathbf{k}\\) is the number of samples with \\(\\mathbf{y}_i=\\mathbf{k}\\).\n\\[\n\\begin{align}\n\\mathbf{\\mu}_\\mathbf{k} &= \\frac{\\sum_{i \\text{  s.t.  } \\mathbf{y}_i=\\mathbf{k}} \\mathbf{x}_i}{n_\\mathbf{k}} \\\\\n\\Sigma_\\mathbf{k} &= \\frac{\\sum_{i \\text{  s.t.  } \\mathbf{y}_i=\\mathbf{k}} (\\mathbf{x}_i-\\mathbf{\\mu}_\\mathbf{k})(\\mathbf{x}_i-\\mathbf{\\mu}_\\mathbf{k})^\\top}{n_\\mathbf{k}} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/20.html#gradient-descent-estimation-of-theta",
    "href": "blog/20.html#gradient-descent-estimation-of-theta",
    "title": "Statistical Learning",
    "section": "Gradient Descent Estimation of \\(\\theta\\)",
    "text": "Gradient Descent Estimation of \\(\\theta\\)\nSince we are using QDAs/GDAs, we have analytical solution available us (and we should use it in practice if QDAs/GDAs is what we really want). But most ML models, especially in vision, uses more complicated formulations like neural networks, we have to resort using numerical optimizations like gradient descent (GD), stochastic gradient descent, and Adam. So let us see how we optimize model in the majority of the cases via GD over the negative log-likelihood (NLL), which is just the negation of the log of the likelihood \\(\\frac{1}{n}\\sum^n_{i=1} p(\\mathbf{y}_i=\\mathbf{k}|\\mathbf{x}_i,\\theta)\\), including the NLL of the Gaussian in def neg_log_likelihood_gaussian. Note, we omit \\(\\log\\phi_\\mathbf{k}\\) since we are not optimizing that.\nThis will take about a few minutes. Make sure you run the previous cell.\n\n\n\n\n\n\nYou can see how the model distribution gets closer to the true data distribution for each gradient step. Remember, each point is a data point representing the 3-pixel image of a traffic light. Theoretically speaking, if there exists an analytical solution of a quadratic expression (single minimum), gradient descent should effectively reach to the same optimal point. However, the reason it‚Äôs not is likely because we‚Äôre missing the class prior \\(\\phi_\\mathbf{k}\\) or the variance (i.e., the spread) is different between the two categories (needing two different learning rate?) Regardless, it still mostly converges."
  },
  {
    "objectID": "blog/40.html",
    "href": "blog/40.html",
    "title": "Interesting Topics For November",
    "section": "",
    "text": "It‚Äôs always good to occasionally take a step back among the hustle and bustle, recalibrate your sense of direction, find new interesting questions to answer, and reflect the progress made from the past year. This blog aims to fossilize my interests at this time for me to potentially start a project from several of these and look back on, and maybe of interest for you too for whatever reason it may be. Note: this is more of an unorganized thought dump.\nMore computer graphics than 3D vision:\n\nPath-space differentiable rendering\nGeometric rendering: black holes\nRendering exotic light sources: cold-cathode displays\nStochastic Geometry\nRe-formulating problems as light-transport problems: Inverse rendering, walk-on-stars\n\nMore 3D vision than computer graphics:\n\nNeural 3D Reconstruction/Generation: VGGT, 3DGS, Diffusion\nDiffusion models & stochastic differential equations (SDEs)\nBundle Adjustment & Lie Algebra\nFast fourier transform (FFT)\nApplications of 3D Vision + graphics: Robotics, Perception, Reinforcement Learning"
  },
  {
    "objectID": "blog/40.html#path-space-differentiable-rendering-psdr",
    "href": "blog/40.html#path-space-differentiable-rendering-psdr",
    "title": "Interesting Topics For November",
    "section": "Path-space differentiable rendering (PSDR)",
    "text": "Path-space differentiable rendering (PSDR)\n\n\n\nImage from a course on physics-based differentiable rendering\n\n\nIn rendering, we take a description of a virtual 3D scene and have the renderer produce a 2D image of it given a camera location and orientation. More specifically, a scene has geometry (e.g., meshes, radiance fields) and light sources (or else the image would just be dark) where the renderer merely records the incoming radiance onto an image. So, an image is merely a record of the more complete virtual 3D scene that is easily transferred and displayed without needing to know the complicated underlying scene (e.g., *.ply files) and executing resource-intensive rendering process (e.g., GPUs). In fact, an image is not limited to recording virtual scenes, but it can also record real-life scenes where we usually don‚Äôt know the underlying complex environment (e.g., given a photo of a mountain, what is the geometry of the mountain?)1. This is where it gets interesting and how PSDR comes into play. Check the other section to see how PSDR is used in this context.\n1¬†But remember, a visual record needs light to mediate the underlying complex scene information. (Think exposure time: darker scenes needs longer time to capture the same amount of light to produce a sufficient visual record of a scene, whereas brighter scenes just needs shorter time. If a scene is completely dark, you can think of it needing infinite amount of time to produce an equal quality of a photo.)In forward rendering, there are mostly two common methods: rasterization and path tracing (and a mix of both). If we formalize these rendering process as a function, we get \\(I=f(\\theta)\\), where \\(I\\) is the final image, \\(\\theta\\) is the scene parameters (e.g., geometry, cameras), and \\(f\\) is the rendering process. What if we can differentiate it? Like \\(I=\\frac{d}{d\\theta}f(\\theta)\\)? Actually, this question is not too far-fetched in that machine learning has revolutionized in how we model intelligent-like behaviors and optimization, and differentiability is the core idea that enables such learning to take place.\nWe will be focusing on the more physically-plausible path tracing (via path-integral formulation). In this form, differentiating the rendering process produces two terms: interior term and boundary term. You can read more about them on Zhang et al. (2020), but basically interior term is what we are mainly differentiating against and the boundary term is an important term that accounts for visibility changes, which commonly appears when we differentiate against geometry.\nThis is why differentiation with respect to the geometry are messy and complicated. For geometric reconstruction, the shapes can change, inducing a visibility changes on a rendered image. For estimating camera pose, same thing: potential visibility changes. For moving geometries, same visibility changes. And what do we mean by visibilty changes anyways? When some geometry goes in front of another, we have an occlusion of the geometry in the back. These changes happen on discontinuities, making the differentiation hard. Hence, there‚Äôs a lot of current actively-explored topics on computing the derivatives in these fundamental areas.\nAnother important categories of parameter that we differentiate against are colors. More precisely, emission strength of light sources and the modulation by BSDF. No geometry means easier interior term and no boundary term, but that doesn‚Äôt mean it is easier.. overall. Sometimes, the light sources have small areas or the BSDF is highly-specular. In these cases, sampling becomes important and there are many active topics in this regards (e.g., variance reduction).\nOriginal paper on PSDR: Zhang et al. (2020)\n\nZhang, Cheng, Bailey Miller, Kai Yan, Ioannis Gkioulekas, and Shuang Zhao. 2020. ‚ÄúPath-Space Differentiable Rendering.‚Äù ACM Trans. Graph. 39 (4): 143:1‚Äì19.\nThere are other interesting questions on generalizing PSDR to different rendering processes and geometries:\n\nParticipating Media: Zhang, Yu, and Zhao (2021)\nVolume Rendering: Yu et al. (2023)\nother cool stuff‚Ä¶\n\n\nZhang, Cheng, Zihan Yu, and Shuang Zhao. 2021. ‚ÄúPath-Space Differentiable Rendering of Participating Media.‚Äù ACM Trans. Graph. 40 (4): 76:1‚Äì15.\n\nYu, Z., C. Zhang, O. Maury, C. Hery, Z. Dong, and S. Zhao. 2023. ‚ÄúEfficient Path-Space Differentiable Volume Rendering with Respect to Shapes.‚Äù Computer Graphics Forum 42 (4).\n\nZeng, Yunfan, Guangyan Cai, and Shuang Zhao. 2025. ‚ÄúA Survey on Physics-Based Differentiable Rendering.‚Äù https://arxiv.org/abs/2504.01402.\nSurvey paper: Zeng, Cai, and Zhao (2025)\nTopics I need to focus more on üòÑ:\n\nReparamaterizations\nSampling theory\nImplementing those scary differentiated version of those algorithms, especially when re-formulating it in terms of OptiX."
  },
  {
    "objectID": "blog/40.html#geometric-rendering-black-holes",
    "href": "blog/40.html#geometric-rendering-black-holes",
    "title": "Interesting Topics For November",
    "section": "Geometric rendering: black holes",
    "text": "Geometric rendering: black holes\n\n\n\nInterstellar (2014)\n\n\nWhen I mean geometric rendering, I specifically mean geometrically interesting light transport. Usually, rendering has always just been straight line, which serves majority of the purposes. And when we generalize the theory of light transport, we usually talk about wave 2 and quantum optics, according to Veach (1998), which is still just straight lines. Yes, mirrors, caustics, and diffraction does make it more interesting, but they still bend at a single point, not continuously. One thing pops up in my mind whenever I think of a light ray that continuously bends: black holes. Their force is so strong that it warps the light rays continuously, not just bend it at a single point.\n\n2¬†The dissertation mentioned how diffraction can occur even around an object. A good example of this is half-plane diffraction, where the lightened side is just bright, but the shadow side still exhibits extra radiance from the bending of the light or even wave-like oscillation under certain conditions. This is different from umbras and penumbras. Took a while to find an example of this effect since most diffraction examples are about apertures and slits, which I feel like is better termed as ‚Äúthrough an object.‚Äù\nVeach, Eric. 1998. ‚ÄúRobust Monte Carlo Methods for Light Transport Simulation.‚Äù PhD thesis, Stanford, CA, USA: Stanford University.\n3¬†There also exists a similarly theory called special relativity, which is a specific case of general relativity. You can read more about here on ¬ß7 of the lecture notes. But basically we assume no gravity (hence, no force, no curvatures, and constant velocities) and focus on the spacetime nature of extreme cases in classical physics (e.g., perceived time at light speed).So, I guess a quick teaser (for me and you) with a demo, I will first create a point of singularity and a light path unaffected by it. What would a light path affected by gravitational force be like? We first need to know the theory behind gravity, and currently there are practically only two out there: Newton‚Äôs law (for most use cases) and general relativity 3 (for exotic cases). You can probably guess which theory contains the black hole.\nI started with ¬ß6: Black Holes, which says there are multiple kind of black holes with the Schwarzschild black hole being the simplest of all. However, this section mostly discuss about proving and understanding the various properties of many kinds of black holes, which will only be useful later on (beyond the scope). ¬ß1.3.5: Light Bending is what we care right now, discussing how light bends under Schwarzschild metric (i.e., the foundation for Schwarzschild black hole). By equation 1.54, we get the trajectory of our light ray affected by the gravitational force to be \\(\\frac{d^2u}{d\\phi^2}+u=\\frac{3GM}{c^2}{u^2}\\), where \\(u(\\phi)=\\frac{1}{r(\\phi)}\\) is the inverse polar radius, \\(G\\) is Newton‚Äôs gravitational constant (it appears again!), and \\(M\\) is the mass of our object warping the light path 4. I will quickly demo this by evaluating the ODE numerically via Euler‚Äôs method.\n4¬†Recall that light has no mass (i.e., null geodesic), so technically it is not orbiting around the black hole, but just following the spacetime frame itself that‚Äôs getting curved by the black hole.I first need to rewrite the second-order ODE \\(\\ddot{u}+u=\\frac{3GM}{c^2}{u^2}\\) (with \\(\\phi\\) as our independent variable) into a systems of first-order ODE for Euler‚Äôs method. \\[\n\\begin{cases}\n\\dot{u}=v \\\\\n\\dot{v}=\\frac{3GM}{c^2}{u^2}-u \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\nAdditional ideas:\n\nGravitational lensing (general case of black holes)\nWormholes\nCollision of two black holes\nRed shift\nRelativistic distortion (i.e., camera near the black holes)\nAccretion disk\nSpinning black holes\nCan we use differentiable rendering on an image of a black hole to estimate various parameter of it (e.g., mass)?\n\nJust out of curiosity (there‚Äôs definitely a better way)\n\n\nTopics to review:\n\nDifferential Geometry\nGeneral Relativity\nNon-euclidean geometry\nNumerical ‚Äúsolution‚Äù to ODE\n\nPersonal opinion: this is very different from the stuff I have done before and feels refreshing, but the math behind it is daunting at the same time.\nWhere graphics and astrophysics meet: paper link"
  },
  {
    "objectID": "blog/40.html#rendering-exotic-light-sources-cold-cathode-displays",
    "href": "blog/40.html#rendering-exotic-light-sources-cold-cathode-displays",
    "title": "Interesting Topics For November",
    "section": "Rendering exotic light sources: cold-cathode displays",
    "text": "Rendering exotic light sources: cold-cathode displays"
  },
  {
    "objectID": "blog/40.html#stochastic-geometry",
    "href": "blog/40.html#stochastic-geometry",
    "title": "Interesting Topics For November",
    "section": "Stochastic geometry",
    "text": "Stochastic geometry\nAs if probability (or this notion of probabilistic stuff) wasn‚Äôt enough, stochasticity (or stochastic stuff) is this new kid down the street I have been hearing quite frequently (apart from stochastic gradient descent). Let us answer first on what is stochasticity, and how is it different from ‚Äúprobabilistic‚Äù?\nAccording to Merriam-Webster, for something to be stochastic, it means the something involves a random variable, chance, or probability, which actually is just ever more slightly specific then the term ‚Äúprobabilistic.‚Äù In other words, they‚Äôre quite same. However, from my initial understanding, people generally associate probabilistic model as something that emphasizes more on learning and inference, whereas stochastic model emphasizes more on modelling the randomness (to a certain degree) of what otherwise would be a mostly deterministic evolutionary process (i.e., stochastic process)."
  },
  {
    "objectID": "blog/40.html#re-formulating-problems-as-light-transport",
    "href": "blog/40.html#re-formulating-problems-as-light-transport",
    "title": "Interesting Topics For November",
    "section": "Re-formulating problems as light-transport",
    "text": "Re-formulating problems as light-transport"
  },
  {
    "objectID": "blog/40.html#neural-3d-reconstructiongeneration",
    "href": "blog/40.html#neural-3d-reconstructiongeneration",
    "title": "Interesting Topics For November",
    "section": "Neural 3D Reconstruction/Generation",
    "text": "Neural 3D Reconstruction/Generation"
  },
  {
    "objectID": "blog/40.html#diffusion-models-stochastic-differential-equations-sdes",
    "href": "blog/40.html#diffusion-models-stochastic-differential-equations-sdes",
    "title": "Interesting Topics For November",
    "section": "Diffusion Models & Stochastic Differential Equations (SDEs)",
    "text": "Diffusion Models & Stochastic Differential Equations (SDEs)"
  },
  {
    "objectID": "blog/40.html#bundle-adjustment-ba-lie-algebra",
    "href": "blog/40.html#bundle-adjustment-ba-lie-algebra",
    "title": "Interesting Topics For November",
    "section": "Bundle Adjustment (BA) & Lie Algebra",
    "text": "Bundle Adjustment (BA) & Lie Algebra"
  },
  {
    "objectID": "blog/40.html#fast-fourier-transform-fft",
    "href": "blog/40.html#fast-fourier-transform-fft",
    "title": "Interesting Topics For November",
    "section": "Fast Fourier Transform (FFT)",
    "text": "Fast Fourier Transform (FFT)"
  },
  {
    "objectID": "blog/40.html#applications-of-3d-vision-graphics",
    "href": "blog/40.html#applications-of-3d-vision-graphics",
    "title": "Interesting Topics For November",
    "section": "Applications of 3D Vision + Graphics",
    "text": "Applications of 3D Vision + Graphics\ngeometrically-interesting light-transports): Black holes (differential geometry + what is homeomorphisms\n(i.e., efficient simulation of PDEs via WoS, inverse rendering, visual pose estimation)"
  },
  {
    "objectID": "blog/40.html#rendering-exotic-light-sources-cold-cathode-fluorescent-displays-ccfds",
    "href": "blog/40.html#rendering-exotic-light-sources-cold-cathode-fluorescent-displays-ccfds",
    "title": "Interesting Topics For November",
    "section": "Rendering exotic light sources: cold-cathode fluorescent displays (CCFDs)",
    "text": "Rendering exotic light sources: cold-cathode fluorescent displays (CCFDs)\n\n\n\nNixie Tubes\n\n\nI am a big fan of retro technologies, especially the warm neon lights of the bygone days. A year ago, I bought this (it‚Äôs a bit pricey) just out of fascination of what it looks like in real life. Unfortunately, it has this distinctive blue overtone which is missing when taking a photo with a consumer device (which makes it all of more interesting). Now, I‚Äôm curious if it‚Äôs possible to replicate this warm fuzzy feeling of a Nixie tube into computer rendering. There are three approaches to this with increasing level of complexity: emissive solid (i.e., a mesh with an emissive material), emissive participating media, or physically accurate simulation of CCFDs. I‚Äôll mostly be approaching it from the emissive participating media and a bit of physically-accurate modelling (i.e., physically-plausible CCFDs).\nSo how does one go about rendering the warm tone of CCFDs? Are they even always in an orange tone? How do they work? I‚Äôve came across this Google Group discussing about a book ‚ÄúCold Cathode Glow Discharge Tubes‚Äù, by GF Weston from 1968, available in PDF which has a wealth of information in how CCFDs work and their physical properties. Unfortunately, there were not a lot of discussion in optical properties (of this book and elsewhere I can find) that are important in rendering CCFDs. It is most likely because these electrical discharges (hence, light) from excited gasses produce all sorts of colors that are very sensitive to various parameters such as gas pressure, cathode-anode distance, voltage, amperage, gas composition, material used in for cathodes/anodes, etc. Since I‚Äôm not here to physically simulate what CCFDs would look like from these various parameters, I‚Äôll be focusing on Nixie tubes (which has more information usually) and maybe a limited range of parameters (it‚Äôs still interesting to see how CCFDs would evolve over certain parameters).\n\n\n\nFig 3.2 of ‚ÄúCold Cathode Glow Discharge Tubes‚Äù\n\n\nCCFDs can just be an emissive participating media with an assigned color of orange, but that‚Äôs no fun, since there are also various color overtones it produces depending on the gas composition that would be missing. Hence, I am having the input to just be the gas composition and some basic geometric model of the energy field (i.e., the emission surrounding the shape of the cathode or the cathode sheathed in a layer of emission5). Unfortunately, it seems like most Nixie Tube discussion lack the information on their visual properties. So, we turn to the physics of light in hopes of simulating the same physically correct emission of gas composition like shown in the picture. To get there, we need to know some radiative transfer theory, which is readily talked about in Pharr, Jakob, and Humphreys (2023, chap. 11) for volumetric scattering process and more completely in Pharr, Jakob, and Humphreys (2023, chap. 14) with the introduction of the radiative transfer equation.\n5¬†Sometimes, the negative glow is shown sticking onto the cathode. In other times, we have two dark spaces with three regions of emission from the cathode side (before the Faraday dark space). The geometries of emission can be quite sensitive, so we will assume the ‚Äúnegative glow‚Äù is sticked/sheathed onto the cathode.6¬†As I look into this & the Nixie Tubes more, there are many more physical abstraction you can uncover (e.g., electrical glow discharge, fluorescent bulbs, anisotropic absorption/out-scattering, etc.), which at a certain point, you‚Äôre just modelling electromagnetic waves themselves. In my opinion, we model the most physically-grounded renderer as much as possible while also keeping it fast, so given some known discrete scene parameters, we can render a realistic image in a computationally-sensible way. I hope this is what graphics is all about.In computer graphics, rendering volumetric effects requires us to at least model three main 6 physical process: absorption, emission, and scattering (further broken down as in-scattering and out-scattering). By assumption, we model these effects as differential equations (ordered as emission, absorption, out-scattering, and in -scattering):\n\\[\n\\begin{align}\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= \\sigma_a(\\mathbf{x},\\mathbf{\\omega}) L_e(\\mathbf{x},\\mathbf{\\omega})dt \\\\\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= -\\sigma_a(\\mathbf{x},\\mathbf{\\omega}) L_i(\\mathbf{x},-\\mathbf{\\omega})dt \\\\\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= -\\sigma_s(\\mathbf{x},\\mathbf{\\omega}) L_i(\\mathbf{x},-\\mathbf{\\omega})dt \\\\\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= \\sigma_s(\\mathbf{x},\\mathbf{\\omega})\\int_{S^2} p(\\mathbf{x},\\mathbf{\\omega},\\mathbf{\\omega}_i)L_i(\\mathbf{x},\\omega_i)d\\omega_i dt \\\\\n\\end{align}\n\\]\nwhere \\(dL_o(\\mathbf{x},\\mathbf{\\omega}):=L_o(\\mathbf{x}+\\mathbf{\\omega}dt, \\mathbf{\\omega})\\) and \\(L_o(\\mathbf{x},\\mathbf{\\omega}):=L_i(\\mathbf{x},-\\mathbf{\\omega})\\) (remember that we are only traversing the volumetric media in a straight line to simplify our model).\n\\(\\sigma_a\\) is the absorption coefficient and \\(\\sigma_s\\) is the extinction coefficient, and \\(p\\) is the phase function‚Äîthe BSDF reciprocal of volumetric scattering.\nSo, if we rewrite the formulas, the combined differential change in radiance at a point \\(\\mathbf{p}'=\\mathbf{p}+t\\mathbf{\\omega}\\) in a volumetric media is \\(\\frac{d}{dt}L_o(\\mathbf{p}',\\mathbf{\\omega})=-\\sigma_t(\\mathbf{p}',\\mathbf{\\omega})L_i(\\mathbf{p}',-\\mathbf{\\omega})+\\sigma_t(\\mathbf{p}',\\mathbf{\\omega})L_s(\\mathbf{p}',\\mathbf{\\omega})\\). The \\(\\sigma_t=\\sigma_a+\\sigma_s\\) is the extinction/attenuation coefficient, and in this case it can be interpreted as the medium‚Äôs density, where larger value means more effect from the medium while lower value means less effect (i.e., transparent air). Notably, the first term includes the transmittance, which can be analytically 7 solved into \\(e^{-\\sigma_t d}\\) where \\(d\\) is the distance 8. Additionally, in Pharr, Jakob, and Humphreys (2023, Equation 11.10), they mysteriously used the indirect \\(T_r(\\mathbf{p}+t\\omega\\to\\mathbf{p}')\\) instead of the more direct \\(T_r(\\mathbf{p}\\to\\mathbf{p}+t\\omega)\\), but are equivalent with a change of variable. This can be thought of finding the optical depth/thickness from \\(0\\to t\\) or remainder of the optical thickness from \\(t \\to d\\) (the inner transmission integral).\n7¬†Assuming homogenous media (i.e., constant density).8¬†This is better known as the exponential volumetric transport, which is commonly seen in NeRF and NeuS. A natural question to ask is whether non-exponential volumetric transport exist? In fact, it does. The volumetric transport we, and the book, implicitly assume is the Beer-Lambert law., whereas non-exponential version opens a whole can of worms. Both all are governed by the generalized radiative transfer equation (RTE).\nPharr, M., W. Jakob, and G. Humphreys. 2023. Physically Based Rendering, Fourth Edition: From Theory to Implementation. MIT Press. https://books.google.ad/books?id=kUtwEAAAQBAJ.\n9¬†As usual, we assume each \\(N\\)-length path has a single emission source at the end, but \\(N\\) can vary from various sampled paths. Just makes the math cleaner without restricting anything.To actually render volumetric effect, we need a volumetric integrator that generalizes the volumetric scattering effect as a path -integral formulation. The general idea is simple: we just generalize the formulation where the domain include medium locations instead of just surface locations and the measure extended to measure positions of volumetric effect. The contribution function is extended to support phase functions (i.e., in-scattering effects), transmittance (i.e., absorption and out-scattering), and emission 9.\n\nAs I‚Äôm writing this out, I also realized clouds is another form volumetric media I want to see it work. Not just a static cloud, but like maybe some procedural noise or even a simple fluid simulation going to see the clouds grow and shrink over time.\nAdditional ideas:\n\nFluid simulation of clouds?\nAccurately render the emission spectrum described from a molecular composition?\n\nTopics to review:\n\nRadiative transfer equation (RTE)\nBeer‚Äôs Lambert Law\nFluid dynamics\nSampling volumetric light transports\n\nPersonal opinion: I think this follows more on traditional computer graphics for good reason, volumetric media is common and knowing it is useful (and it‚Äôs also fun).\nOriginally wanted to model some exotic light sources, but realized they are quite a complex physical process. This section (i.e., the topic) is probably more aptly named as volumetric light transport. Good thing I did this so I have a better idea what the project could look like."
  }
]