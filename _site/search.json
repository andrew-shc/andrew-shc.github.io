[
  {
    "objectID": "blog_entry.html",
    "href": "blog_entry.html",
    "title": "",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFoggy Cornell Box\n\n\nMicro Blog: Volumetric Path Tracer\n\n\n\nFeb 15, 2026\n\n\n\n\n\n\n\n\n\n\n\nMicro Blogs\n\n\nMeta: Micro Blog\n\n\n\nJan 26, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteresting Topics For November\n\n\nGeneral\n\n\n\nNov 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Learning\n\n\nA Review on Computer Vision Fundamentals\n\n\n\nNov 13, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/30.html",
    "href": "blog/30.html",
    "title": "Micro Blogs",
    "section": "",
    "text": "I intend this new format of Micro Blogs to be a more sustainable, consistent ways for me to continue the effort of blogging in a low-barrier way, where I can still keep my definition and standard of what a full blog should be without putting all my time into a single blog. For the past few weeks, I was working on a full blog on FFT (Fast Fourier Transform) for images, since people in-person and videos from online said this is some cool algorithm, which it is, but after spending so much time to find the best way to present this as a blog, I just realized 1D FFT has already been widely talked about, 2D FFT requires a lot of effort that does not exactly match my priorities 1, and animating Fourier Analysis was not on my top top priorities. Last few days were mostly me just mulling whether I should still resume after spending some considerable amount of time on it 2 or just bail it and just directly focus on what I want to do, which I decided latter. The next blog should be visible on my homepage.\n1¬†thought about devising a way to use 4th-rank tensors for the linear transformation between 2D input and 2D output without using the separability of the basis or flattening the dimensions2¬†lowkey I made the scope a bit too large with theoretical generalization of its domain (e.g., Locally-compact Abelian Groups), Laplace and z-transform, and Spherical HarmonicsAs for writing this, I just want a place to clarify what Micro Blogs intends to do and how it differs to full blogs, which I will write one if I ever notice a consistent topics I have been interested and written about it as Micro Blogs. Also, it‚Äôs a way for me at least to write something after bailing the full blog for FFT, which was very unsatisfying (not like the alternatives are any better.)\nHere‚Äôs a fun debug rendering of some stuff I have been working on. I might create an art gallery of debug renders since they have minimal value scientifically but fun to interpret and guess where these errors in the rendering could have arisen and why it had rendered in that way.\n\n\n\nRendering of an unvalidated (likely erroneous) Jacobian at an attempt on differentiable rendering; scaled to a certain factor"
  },
  {
    "objectID": "blog/10.html",
    "href": "blog/10.html",
    "title": "Foggy Cornell Box",
    "section": "",
    "text": "Foggy rotating Cornell box best viewed in dark mode with an eye-watering sampling rate of \\(512^2=262,144\\) sample per pixel.\nAn attempt at creating the most basic volumetric path tracer: Cornell box in foggy atmosphere that is uniformly diffused within the air. Overall concepts will be based on Pharr, Jakob, and Humphreys (2023, chap. 11) and Pharr, Jakob, and Humphreys (2023, chap. 14)."
  },
  {
    "objectID": "blog/10.html#transmittance",
    "href": "blog/10.html#transmittance",
    "title": "Foggy Cornell Box",
    "section": "Transmittance",
    "text": "Transmittance\nFirst, let us look into the extinction portion of the volumetric effects: \\(dL_o(\\mathbf{x}, \\omega) = -\\sigma_\\text{t}(\\mathbf{x}, \\omega) L_i(\\mathbf{x}, -\\omega)dt\\), which is more explicitly written as \\(dL_o(\\mathbf{x}+t\\omega, \\omega) = -\\sigma_\\text{t}(\\mathbf{x}+t\\omega, \\omega) L_i(\\mathbf{x}+t\\omega, -\\omega)dt\\). When \\(t=0\\), we are at \\(\\mathbf{x}\\). whereas we denote the location of \\(\\mathbf{x}'\\) along the direction \\(\\omega\\) when the ray has traversed to time \\(t\\).\n\n\\[\\begin{align}\n\ndL_o(\\mathbf{x}+t\\omega, \\omega) &= -\\sigma_\\text{t}(\\mathbf{x}+t\\omega, \\omega) L_i(\\mathbf{x}+t\\omega, -\\omega)dt \\\\\ndL_o(\\mathbf{x}+t\\omega, \\omega) &= -\\sigma_\\text{t}(\\mathbf{x}+t\\omega, \\omega) L_o(\\mathbf{x}+t\\omega, \\omega)dt\n\n\\end{align}\\]\nFor the sake of brevity, we will abbreviate the notation and re-introduce them later. Pro tip: always use definite integral; indefinite integral causes a lot of confusion in these complicated settings, especially with nested variables like here.\n\\[\\begin{align}\n\ndL_o &= -\\sigma_\\text{t} L_o dt \\\\\n\\frac{1}{L_o} \\frac{dL_o}{dt} &= -\\sigma_\\text{t} \\\\\n\\int^{t=d}_{t=0} \\frac{1}{L_o} \\frac{dL_o}{dt} dt &= \\int^{t=d}_{t=0} -\\sigma_\\text{t} dt \\\\\n\\int^{L_o(t=d)}_{L_o(t=0)} \\frac{dL_o}{L_o} &= \\int^{t=d}_{t=0} -\\sigma_\\text{t} dt \\\\\n\\ln{|L_o(t=d)|}-\\ln{|L_o(t=0)|} &= \\int^{t=d}_{t=0} -\\sigma_\\text{t} dt \\\\\n\\frac{L_o(t=d)}{L_o(t=0)} &= e^{\\int^{t=d}_{t=0} -\\sigma_\\text{t} dt} \\\\\n\\frac{L_o(\\mathbf{x}+d\\omega, \\omega)}{L_o(\\mathbf{x}, \\omega)} &= e^{-\\int^{t=d}_{t=0} \\sigma_\\text{t} dt} \\\\\n\\frac{L_o(\\mathbf{x}', \\omega)}{L_o(\\mathbf{x}, \\omega)} &= e^{-\\int^{t=d}_{t=0} \\sigma_\\text{t} dt} \\\\\n\n\\end{align}\\]\nThis perfectly matches the book‚Äôs definition of transmittance \\(T_r(\\mathbf{x}\\to\\mathbf{x}'):=\\frac{L_o(\\mathbf{x}', \\omega)}{L_o(\\mathbf{x}, \\omega)}\\) where it is always between \\(0\\) and \\(1\\)6, which will be important for our rendering algorithm as a nice way to encapsulate the idea of absorption and out-scattering as an accumulated transmittance on a point along a ray. This is better related to an idea known as attenuation. Additionally, the exponent in the transmittance, \\(-\\int^{t=d}_{t=0} \\sigma_\\text{t} dt\\), refers to optical thickness/depth, which just tells us how much the light is scattered or absorbed in total (linearly) different from the exponentiated version that tells us what the actual final radiance produced.\n6¬†Since \\(\\mathbf{x}'\\) represents a point later, or further away, from the light source then \\(\\mathbf{x}\\), its outgoing radiance must at most be the same or reduced, but not negative. Hence, \\(T_r\\in [0,1]\\). Can also be thought of as the fraction between the radiance of the earlier \\(\\mathbf{x}\\) and the later \\(\\mathbf{x}'\\)."
  },
  {
    "objectID": "blog/10.html#the-equation-of-transfer",
    "href": "blog/10.html#the-equation-of-transfer",
    "title": "Foggy Cornell Box",
    "section": "The Equation of Transfer",
    "text": "The Equation of Transfer\nTransmittance only talks about all the processes that involves reducing the radiance, which is only a part of it. What about the rest? One neat thing about all the assumed equations‚Ä¶ they all modify/change the same underlying \\(L_o\\)! So, we can aggregate each of these individual processes into one diff. eq.: \\(dL_o(\\mathbf{x}+t\\omega, \\omega) = -\\sigma_\\text{t}(\\mathbf{x}+t\\omega, \\omega) L_i(\\mathbf{x}+t\\omega, -\\omega)dt + \\sigma_\\text{t}(\\mathbf{x}+t\\omega, \\omega) L_s(\\mathbf{x}+t\\omega, \\omega)dt\\) to be integrated. As usual, we will abbreviate the notation for brevity. When the ray \\((\\mathbf{x},\\omega)\\) intersects a surface, we will denote the location of intersection as \\(\\mathbf{x}_s\\) in which the ray has traveled \\(t=d\\).\n\\[\\begin{align}\n\ndL_o &= -\\sigma_\\text{t} L_idt + \\sigma_\\text{t} L_sdt \\\\\ndL_o &= -\\sigma_\\text{t} L_odt + \\sigma_\\text{t} L_sdt \\\\\n\\frac{dL_o}{dt} + \\sigma_\\text{t} L_o &= \\sigma_\\text{t} L_s \\\\\n(e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\frac{dL_o}{dt} + (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\sigma_\\text{t} L_o &= (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\sigma_\\text{t} L_s \\\\\n(e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\frac{dL_o}{dt} + (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) (\\frac{d}{dt} \\int^{s=t}_{s=0}\\sigma_\\text{t} ds) L_o &= (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\sigma_\\text{t} L_s \\\\\n\\int^{t=d}_{t=0} \\frac{d}{dt} (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds} L_o) dt &= \\int^{t=d}_{t=0} (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\sigma_\\text{t} L_s dt \\\\\ne^{\\int^{s=d}_{s=0} \\sigma_\\text{t} ds} L_o(t=d) - e^{\\int^{s=0}_{s=0} \\sigma_\\text{t} ds} L_o(t=0) &= \\int^{t=d}_{t=0} (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\sigma_\\text{t} L_s dt \\\\\nL_o(t=d)  &= e^{-\\int^{s=d}_{s=0} \\sigma_\\text{t} ds} L_o(t=0) + e^{-\\int^{s=d}_{s=0} \\sigma_\\text{t} ds} \\int^{t=d}_{t=0} (e^{\\int^{s=t}_{s=0} \\sigma_\\text{t} ds}) \\sigma_\\text{t} L_s dt \\\\\nL_o(t=d)  &= e^{-\\int^{s=d}_{s=0} \\sigma_\\text{t} ds} L_o(t=0) + \\int^{t=d}_{t=0} (e^{-\\int^{s=d}_{s=t} \\sigma_\\text{t} ds}) \\sigma_\\text{t} L_s dt \\\\\nL_o(t=d)  &= T_r(\\mathbf{x}\\to\\mathbf{x}_s) L_o(t=0) + \\int^{t=d}_{t=0} T_r(\\mathbf{x}'\\to\\mathbf{x}_s) \\sigma_\\text{t} L_s dt\n\n\\end{align}\\]\nHowever, this is calculated in the direction towards the point of intersection starting from \\(\\mathbf{x}\\). For the sake of convention, we will reformulate this as a ray marching towards the surface intersection but pointing back to \\(\\mathbf{x}\\) by negating the direction \\(\\omega\\) as a way to compute the incoming radiance \\(L_i\\) at \\(\\mathbf{x}\\) from the surface intersection (and all the volumetric effects that occurs in-between). The equation can be intuitively understood as summing the incremental volumetric effects from the attenuated incoming radiance along the ray (i.e., line integral) and the additional effects from the surface scattering attenuated at the entire length. 7\n7¬†Note, the book uses \\(\\sigma_\\text{t}(\\mathbf{x}',\\omega)\\) with the un-negated direction \\(\\omega\\) likely from different parameterization. However, as long as the attenuation coefficient is directionally symmetric, \\(\\sigma_\\text{t}(\\mathbf{x}',\\omega)=\\sigma_\\text{t}(\\mathbf{x}',-\\omega)\\), which occurs in most basic volumetric media, then this discrepancy can be ignored with discretion.The Equation of Transfer (Integral Form):\n\\[\nL_i(\\mathbf{x},\\omega) = T_r(\\mathbf{x}_s\\to\\mathbf{x}) L_o(\\mathbf{x}_s,-\\omega) + \\int^{t=d}_{t=0} T_r(\\mathbf{x}'\\to\\mathbf{x}) \\sigma_\\text{t}(\\mathbf{x}',-\\omega) L_s(\\mathbf{x}',-\\omega) dt\n\\]\nAnother important idea is when there are no surface scattering along the path the ray travels. Then, \\(d\\to\\infty\\), \\(\\mathbf{x}_s\\) is undefined, and no surface scattering at \\(d\\) to emit the first term of The Equation of Transfer."
  },
  {
    "objectID": "blog/10.html#quick-tangent-variance-reduction-of-l_i",
    "href": "blog/10.html#quick-tangent-variance-reduction-of-l_i",
    "title": "Foggy Cornell Box",
    "section": "Quick Tangent: Variance Reduction of \\(L_i\\)",
    "text": "Quick Tangent: Variance Reduction of \\(L_i\\)\nDisclaimer: it did not work out at the end. The variance actually increases since when doing this, if it hit‚Äôs the lower probability, it adds a multiplicative factor that makes the value vary more as the probability goes to the extrema (i.e., \\(\\to 0\\) or \\(\\to 1\\)). Feel free to skip this sub-section of my attempt at variance reduction. 10 11\n10¬†Pharr, Jakob, and Humphreys (2023, sec. 14.1.2) actually discussed about this issue similarly. However, they used null scattering technique which seems like to reduce the variance. However, despite us assuming homogenous medium, we will defer this for a later micro blog for progression sake.11¬†With these analysis on variance reduction between various random variables, there‚Äôs a potential issue with correlation that can negatively affect the analysis, which we won‚Äôt bother for now.Suppose the following:\n\\[\nL_i' = \\begin{cases}\n\\displaystyle L_e + \\int_{H^2(\\mathbf{n})}fL_i|\\cos(\\theta_i)|d\\omega_i & \\text{with probability } T_r(\\mathbf{x}_s\\to\\mathbf{x})  \\\\\n\\displaystyle \\frac{\\int^{t=d}_{t=0} T_r(\\mathbf{x}'\\to\\mathbf{x}) \\bigg(\\sigma_aL_e+\\sigma_s\\int_{S^2}pL_id\\omega_i\\bigg) dt}{1-T_r(\\mathbf{x}_s\\to\\mathbf{x})} & \\text{otherwise}\n\\end{cases}\n\\]\nWe can show this is unbiased by expectation of sums of random variable (treating each of the two terms as random variable over all path samplings) and linearity of expectation. But does it reduce the variance? Let us analyze it.\nLet us additionally suppose the uniform sampling case for comparison:\n\\[\nL_i'' = \\begin{cases}\n\\displaystyle 2T_r(\\mathbf{x}_s\\to\\mathbf{x})(L_e + \\int_{H^2(\\mathbf{n})}fL_i|\\cos(\\theta_i)|d\\omega_i) & \\text{with probability } \\frac{1}{2}  \\\\\n\\displaystyle 2\\int^{t=d}_{t=0} T_r(\\mathbf{x}'\\to\\mathbf{x}) \\bigg(\\sigma_aL_e+\\sigma_s\\int_{S^2}pL_id\\omega_i\\bigg) dt & \\text{otherwise}\n\\end{cases}\n\\]\nLet \\[\\begin{multline}\n\\\\\nT:=T_r(\\mathbf{x}_s\\to\\mathbf{x}) \\\\\nA:=L_e+\\int_{H^2(\\mathbf{n})}fL_i|\\cos(\\theta_i)|d\\omega_i \\\\\nB:=\\int^{t=d}_{t=0} T_r(\\mathbf{x}'\\to\\mathbf{x}) \\bigg(\\sigma_aL_e+\\sigma_s\\int_{S^2}pL_id\\omega_i\\bigg) dt\n\\end{multline}\\]\nWe can find the variance of the uniform sampling as follows:\n\\[\\begin{align}\n\n\\mathrm{Var}(L_i'') &= \\mathbb{E}[L_i''^2]-\\mathbb{E}[L_i'']^2 \\\\\n&= \\frac{1}{2}\\mathbb{E}[2AT]^2+\\frac{1}{2}\\mathbb{E}[2B]^2-(\\frac{1}{2}\\mathbb{E}[2AT]+\\frac{1}{2}\\mathbb{E}[2B])^2 \\\\\n&= \\frac{1}{2}\\mathbb{E}[2AT]^2+\\frac{1}{2}\\mathbb{E}[2B]^2-\\frac{1}{4}\\mathbb{E}[2AT]^2-\\frac{1}{4}\\mathbb{E}[2B]^2-\\frac{1}{2}\\mathbb{E}[2AT]\\mathbb{E}[2B] \\\\\n&= \\frac{1}{4}\\mathbb{E}[2AT]^2+\\frac{1}{4}\\mathbb{E}[2B]^2-\\frac{1}{2}\\mathbb{E}[2AT]\\mathbb{E}[2B] \\\\\n&= \\mathbb{E}[AT]^2+\\mathbb{E}[B]^2-2\\mathbb{E}[AT]\\mathbb{E}[B]\n\n\\end{align}\\]\nIt seems like something \\(\\mathrm{Var}(L_i)\\) would have at the end‚Äìas in the variance if we had computed the original expression with branched recursion. Now, what about the proposed method in reducing the variance?\n\\[\\begin{align}\n\n\\mathrm{Var}(L_i') &= \\mathbb{E}[L_i'^2]-\\mathbb{E}[L_i']^2 \\\\\n&= T\\mathbb{E}[A]^2+(1-T)\\mathbb{E}\\bigg[\\frac{B}{1-T}\\bigg]^2-\\bigg(T\\mathbb{E}[A]+(1-T)\\mathbb{E}\\bigg[\\frac{B}{1-T}\\bigg]\\bigg)^2 \\\\\n&= T\\mathbb{E}[A]^2+(1-T)\\mathbb{E}\\bigg[\\frac{B}{1-T}\\bigg]^2-T^2\\mathbb{E}[A]^2-(1-T)^2\\mathbb{E}\\bigg[\\frac{B}{1-T}\\bigg]^2-2T(1-T)\\mathbb{E}[A]\\mathbb{E}\\bigg[\\frac{B}{1-T}\\bigg] \\\\\n&= \\frac{\\mathbb{E}[TA]^2}{T}-\\mathbb{E}[TA]^2+\\frac{(1-T)}{(1-T)^2}\\mathbb{E}[B]^2-\\frac{(1-T)^2}{(1-T)^2}\\mathbb{E}[B]^2-2\\mathbb{E}[TA]\\mathbb{E}[B] \\\\\n&= \\frac{1-T}{T}\\mathbb{E}[TA]^2+\\frac{T}{(1-T)}\\mathbb{E}[B]^2-2\\mathbb{E}[TA]\\mathbb{E}[B]\n\\end{align}\\]\nFor the sake of comparison, we ignore the last term and assume \\(\\mathbb{E}[TA]\\) and \\(\\mathbb{E}[B]\\) are the same. This supposedly ‚Äúvariance reduction‚Äù actually only reaches the minimum variance at \\(T=0.5\\) (\\(T\\) for transmittance). In other words, the uniform sampling actually performs better than this method üò≥. While this was a fun tangent to the main goal, this also tells us that not all probabilistic conditional branching like here between the two terms in the recursion can be variance reduced for all probability (e.g., \\(T\\)), especially when the variance expression grows inversely at both ends ü§¶. Unlike Russian roulette where it only inversely grows on one end and the probability is a set constant (i.e., hyperparameter). We will leave it as uniform sampling and leaving this here for posterity."
  },
  {
    "objectID": "blog/10.html#continuation-solving-recursion-with-operators",
    "href": "blog/10.html#continuation-solving-recursion-with-operators",
    "title": "Foggy Cornell Box",
    "section": "Continuation: Solving recursion with operators",
    "text": "Continuation: Solving recursion with operators\nA well-defined path is a finite path. With any finite path, it has a defined length \\(N\\) by \\(N-1\\) bounces with \\(N+1\\) vertices and, logically, has two ends (i.e., no branching). 12 One end always correspond to its respective pixel location \\(\\mathbf{x}_p\\) of the sensor‚Äôs output image. What‚Äôs the point of sampling a path that never reaches to our sensor for you or others to see? For the other end, we should aim for sampling emissive sources \\(\\mathbf{x}_l\\) ordered by its radiance strength. Rendering is really not rendering if there‚Äôs no light üò≤ to spatially mediate the information of a scene. For NEE, we actually do use simple branching at every vertices except the last (I lied; I‚Äôm very sorry), but the branched ends is at most a length of \\(1\\).\n12¬†It can still physically travel infinitely where the path goes on forever pointing towards the sky/space (assuming no atmospheric scattering), but that breaks our algorithm with our finite amount of computational resources. Hence, the path is defined by finite amount of bounces (i.e., interesting scattering events), which can still be flexible in modelling reality.13¬†In Pharr, Jakob, and Humphreys (2023, sec. 14.2.1), their provided algorithm for \\(L_i\\) notably shows a random selection between three terms for evaluation per Pharr, Jakob, and Humphreys (2023, Equation 14.5): volumetric emission, in-scattering, and null scattering. Since we assume no volumetric emission and null scattering, we will evaluate each term in the source function \\(L_s\\) as usual. Volumetric emission is left for completion sake and consistency from previous derivation, but will be removed at the right time. In fact, in their VolPathIntegrator, they re-added the absorption/emission case as always sampled/evaluated along the ray in the medium.14¬†For VolPathIntegrator in Pharr, Jakob, and Humphreys (2023), their probabilistic conditional branching between inhomogeneous medium scattering and surface scattering works by whether the medium has sampled a null scattering event (neither terminated nor scattered). This makes sense. If there‚Äôs some empty area in the medium, it will likely ‚Äúhit‚Äù some null scattering, and given enough luck, it can travel to the end of the medium and hit a surface. However, we‚Äôre not assuming inhomogeneous medium and not delta tracking method (for now), so we just uniform sample between surface intersection and medium intersection.\nPharr, M., W. Jakob, and G. Humphreys. 2023. Physically Based Rendering, Fourth Edition: From Theory to Implementation. MIT Press. https://books.google.ad/books?id=kUtwEAAAQBAJ.\nIf we rewrite our newly formed recursion as the following‚Ä¶1314 \\[\nL_i = \\begin{cases}\n\\displaystyle 2T_r(\\mathbf{x}_s\\to\\mathbf{x})(L_e + \\int_{H^2(\\mathbf{n})}fL_id\\sigma^\\perp(\\omega_i)) & \\text{with probability } \\frac{1}{2}  \\\\\n\\displaystyle 2\\int^{t=d}_{t=0} T_r(\\mathbf{x}'\\to\\mathbf{x}) \\bigg(\\sigma_aL_e+\\sigma_s\\int_{S^2}pL_id\\sigma(\\omega_i)\\bigg) dt & \\text{otherwise}\n\\end{cases}\n\\]\n\nHowever, it‚Äôs the best if we can convert this form into a general path space integral formulation to potentially apply variance reduction techniques and better analyze which variables (e.g., throughput) to track across scattering events.15 To unravel this recursion, we actually abstract the equation an order more via functional analysis to better see the bigger picture and use the tools from them to analytically solve the recursion! We can treat the output quantity of the recursion as an expectation to also abstract the notion of probability away. Let‚Äôs start with the abstraction first with operators:\n15¬†According to many sources, path space integral formulation allows us to unify various formulation of path tracing (e.g., ‚Äúthe rendering equation‚Äù), enabling the application of advanced variance reduction techniques (e.g., bidirectional path tracer) and some other exotic stuff. These are definitely beyond the scope, but it‚Äôs on our radar.We have to abstract emission first. \\(S\\) is a set of points (parameterized two-dimensionally) that defines a surface in our scene. \\[\n(\\mathcal{E}h)(\\mathbf{x},\\omega) = \\begin{cases}\nL_e(\\mathbf{x},\\omega) & \\mathbf{x}\\in S  \\\\\n\\sigma_a(\\mathbf{x},\\omega)L_e(\\mathbf{x},\\omega) & \\text{otherwise}\n\\end{cases}\n\\]\nFollowing Jakob (2013, sec. 3.2), we define the scattering operator \\(\\mathcal{K}\\) and propagation operator \\(\\mathcal{G}\\) as follows.\n\\[\n(\\mathcal{K}h)(\\mathbf{x},\\omega) := \\begin{cases}\n\\displaystyle \\int_{H^2(\\mathbf{n})}f(\\mathbf{x},\\omega_i\\to\\omega)h(\\mathbf{x},\\omega_i)d\\sigma^\\perp_\\mathbf{x}(\\omega_i) & \\mathbf{x}\\in S  \\\\\n\\displaystyle \\sigma_s(\\mathbf{x},\\omega)\\int_{S^2}p(\\mathbf{x},\\omega_i\\to\\omega)h(\\mathbf{x},\\omega_i)d\\sigma_\\mathbf{x}(\\omega_i) & \\text{otherwise}\n\\end{cases}\n\\]\nSince \\(f\\leq1\\), \\(p\\leq1\\), \\(\\sigma_s\\leq1\\), \\(d\\sigma^\\perp_\\mathbf{x}(\\omega_i)\\leq1\\), and \\(d\\sigma^\\perp_\\mathbf{x}(\\omega_i)&lt;1\\), the operator norm of \\(\\mathcal{K}\\) is less than or equal to \\(1\\) (i.e., \\(||\\mathcal{K}||\\leq 1\\)). 16\n16¬†This is more talked about in Veach (1998, sec. 4.2), where this and the next operator are actually operated on a space of integrations between two functions on ray space. This space can be further constructed as the Banach space (i.e., \\(L_p\\) space), or further down as the Hilbert space (i.e., inner product space), which allows the construction of operator norms.\nVeach, Eric. 1998. ‚ÄúRobust Monte Carlo Methods for Light Transport Simulation.‚Äù PhD thesis, Stanford, CA, USA: Stanford University.\n\\[\n(\\mathcal{G}h)(\\mathbf{x},\\omega) := \\begin{cases}\n\\displaystyle 2T_r(\\mathbf{x}_s\\to\\mathbf{x})h(\\mathbf{x}_s,-\\omega) &  \\text{with probability } \\frac{1}{2}  \\\\\n\\displaystyle 2\\int^{t=d}_{t=0} T_r(\\mathbf{x}'\\to\\mathbf{x}) h(\\mathbf{x}',-\\omega) dt & \\text{otherwise}\n\\end{cases}\n\\]\nHere, \\(T_r\\leq1\\); hence, \\(||\\mathcal{G}||\\leq1\\).\nAll three operators are can be merely thought of as macros, lambdas, or higher-order functions that operates at a level more abstract, where it takes in a function and outputs a modified function. Collectively, \\(\\mathcal{K}\\) and \\(\\mathcal{G}\\) can be thought of as the transport operators, since they merely transport the radiance and follow the conservation law of energy. Hence, the recursion can be simplified as follows, which strips away branching and integral expression details for the more important parts.\n\\[\nL_i = \\mathcal{G}(\\mathcal{K}L_i + \\mathcal{E}L_e)\n\\]\nNow, we solve the recursion. 17\n17¬†If you do the other way, you might get \\(L_i=(\\mathcal{G}^{-1} - \\mathcal{K})^{-1}\\mathcal{E}L_e\\), which is more complicated, or even impossible, to solve.\\[\n\\begin{align}\nL_i = \\mathcal{G}\\mathcal{K}L_i + \\mathcal{G}\\mathcal{E}L_e &\\implies L_i - \\mathcal{G}\\mathcal{K}L_i= \\mathcal{G}\\mathcal{E}L_e \\\\\n&\\implies (\\mathcal{I} - \\mathcal{G}\\mathcal{K})L_i= \\mathcal{G}\\mathcal{E}L_e \\\\\n&\\implies L_i= (\\mathcal{I} - \\mathcal{G}\\mathcal{K})^{-1}\\mathcal{G}\\mathcal{E}L_e\n\\end{align}\n\\]\nGreat! It‚Äôs no longer a recursion, but‚Ä¶ how do we even compute the inverse of an operator \\((\\mathcal{I} - \\mathcal{G}\\mathcal{K})^{-1}\\)? (We‚Äôre not even talking about the function themselves anymore.) Neumann series has a solution for us, which is a formula that extends the convergence of geometric series (over scalars) to over operators. However, it requires \\(||\\mathcal{G}\\mathcal{K}||&lt;1\\).\nThis is easy. Recall \\(||\\mathcal{G}||\\leq1\\) and \\(||\\mathcal{K}||\\leq1\\). The only time either is \\(1\\) is when the scene is perfectly vacant and probably some other degenerative cases that we can ignore. Out of practicaly, we can assume that‚Äôs not the case and we can apply the Nuemann series to get the following.\n\\[\nL_i= \\sum_{k=0}^\\infty (\\mathcal{G}\\mathcal{K})^k \\mathcal{G}\\mathcal{E}L_e\n\\]\nWe started with the differential equations and integro-differential form that closely describes the physical processes of volumetric scattering at a local level. Then, we re-express them into the recursive integral formulation‚Äìthe equation of transfer‚Äìwhich prepares for the global light transport. The operators helps us re-express, again, the equation of transfer as a summation of increasing path length. Currently, this global light transport does not restrict the start/end (doesn‚Äôt matter) of the path with the sensor, which was briefly mentioned in the first paragraph of this section regarding \\(\\mathbf{x}_p\\). We will address this in the next section with the sensor measurement integral that fixes the global light transport to sensors, allowing you to actually render the scene to your screen."
  },
  {
    "objectID": "blog/10.html#evaluating-path-integrals",
    "href": "blog/10.html#evaluating-path-integrals",
    "title": "Foggy Cornell Box",
    "section": "Evaluating path integrals",
    "text": "Evaluating path integrals\nLet \\(I_j\\) be our per-pixel radiance measurement where \\(j\\) refers to the pixel index where we want our radiance to be recorded to. Per Jakob (2013, sec. 3.4), \\(I_j:=\\langle W_e^{(j)},L_i\\rangle\\). We eventually get \\(\\displaystyle I_j=\\int_\\Omega f_j(\\overline{x})d\\mu(\\overline{x})\\). You can read the sources more for the notation, but \\(\\Omega\\) is the domain of all path lengths \\(N\\) 18 from \\(0\\) to \\(\\infty\\) bounces with each containing all \\(2^N\\) combination of volumetric and surface scattering all eagerly evaluated (instead of the expectation, the integration evaluates all possible branches, theoretically). There are several key ideas: they let \\((\\mathcal{G}\\mathcal{K})^k\\mathcal{G}=\\mathcal{G}(\\mathcal{K}\\mathcal{G})^k\\), where \\(\\mathcal{K}\\mathcal{G}\\) allows easier simplification. In volumetric scattering, given some integrand \\(h\\), we can simplify it as follows: \\(\\displaystyle \\int_0^d\\int_{S^2} hd\\sigma_\\mathbf{x}(\\omega)dt=\\int_0^d\\int_{S^2} \\frac{h}{t^2}dA(\\mathbf{y})dt=\\int_V \\frac{h}{t^2}dV(\\mathbf{y})\\)19.\n\nJakob, Wenzel Alban. 2013. Light Transport on Path-Space Manifolds. Cornell University.\n18¬†\\(N\\) can be finite (instead of \\(\\infty\\) per Neumann series) because at a certain point the ratios will be smaller than the precision of common numerical datatypes on GPU.19¬†Also mentioned in Zhang (2022, Equation 2.19)\nZhang, Cheng. 2022. ‚ÄúPath-Space Differentiable Rendering.‚Äù eScholarship, University of California, October. https://escholarship.org/uc/item/25q5562p.\nWe are now getting very close to the final form that prepares us to finally render our foggy atmosphere! According to the sources (both the book and the thesis)‚Ä¶\n\\[\n\\begin{align}\nf_j(\\overline{\\mathbf{x}}) &= f_j(\\mathbf{x}_p,\\mathbf{x}_{N-1},\\cdots,\\mathbf{x}_{2},\\mathbf{x}_1) \\\\\n&= \\hat{L_e}(\\mathbf{x}_{1}\\to\\mathbf{x}_{2})\\bigg[\\prod^{N-1}_{n=2}\\hat{G}(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n})\\hat{f}(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n}\\to\\mathbf{x}_{n+1})\\bigg]\\hat{G}(\\mathbf{x}_{N-1}\\to\\mathbf{x}_{N})W_e^{(j)}(\\mathbf{x}_{N-1}\\to\\mathbf{x}_{N})\n\\end{align}\n\\]\nA lot of the notation here is pretty standard and the hats (\\(\\hat{\\cdot}\\)) just represents the emission function \\(L_e\\), scattering function \\(f\\), and geometry function \\(G\\) are their generalized counterpart of surface-only transport. They are evaluated slightly differently when interacting with volumetric points.\nA single sample of Monte-Carlo evaluation of the measurement integral:\n\\[\nI_j = \\frac{\\hat{L_e}(\\mathbf{x}_{1}\\to\\mathbf{x}_{2})\\bigg[\\prod^{N-1}_{n=2}\\hat{G}(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n})\\hat{f}(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n}\\to\\mathbf{x}_{n+1})\\bigg]\\hat{G}(\\mathbf{x}_{N-1}\\to\\mathbf{x}_{N})W_e^{(j)}(\\mathbf{x}_{N-1}\\to\\mathbf{x}_{N})}{\\text{pdf}(\\overline{\\mathbf{x}})}\n\\]\nRecall that for each path, we uniformly branch between surface scattering and volumetric scattering, which be simplified away (we implicitly included a factor of \\(2\\) in the previous equations). However, this means the PDF at each vertices depends on the branching. Additional, if \\(\\mathbf{x}_p=\\mathbf{x}_N\\), the sensor is a surface (i.e., we usually render on a flat rectangular screen), and we pre-determine the light ray per pixel to ensure each pixels is sampled, we also assume \\(\\mathbf{x}_{N-1}\\) is given (i.e., not sampled). We also assume \\(\\hat{L_e}\\) is an area light source on a surface (not volumetric), which means we sample it on a surface and its PDF is the inverse of its surface area \\(\\text{pdf}_A(\\mathbf{x}_1)\\). This remains us with \\(\\prod_{k=2}^{N-2} \\widehat{\\text{pdf}}(\\mathbf{x}_k)\\).\nImportantly, to make the computation of PDF practical and closer to the algorithm than the theory, we use cosine-weighted BSDF sampling, simplifying \\(\\hat{G}\\) term and the PDF20 to just the transmittance \\(T\\). When we branch to volumetric scattering, we will additionally sample the position along a lienar path from \\(k\\)th vertex to the next surface intersection. We have to additionally incorporate its PDF as \\(\\frac{1}{t_k}\\) where \\(t_k\\) is the length between the two vertices. We denote \\(\\frac{1}{\\hat{t_k}}\\) where it will just be \\(1\\) when branched to the surface intersection. For the volumetric scattering, they are full spherical integration, not hemisphere. Additionally, we will be assuming istropic scattering. Therefore, the phase function \\(p\\) would be \\(p(\\mathbf{x},\\omega_i\\to\\omega_o)=\\frac{1}{4\\pi}\\) 21 and its PDF would be \\(\\frac{1}{4\\pi}\\).\n20¬†The term, \\(\\hat{G}(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n})=G(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n})T(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n})\\), is an extension of the regular surface-only \\(G\\) term. Since we are directly sampling the ray direction \\(\\omega\\), the \\(\\frac{1}{r^2}\\) (or \\(\\frac{1}{(\\mathbf{x}-\\mathbf{y})^2}\\)) factor is omitted. When we further sample the direction directly via cosine-weighting, we can entirely omit the \\(\\cos\\) term on either side (recall, \\(G=\\frac{\\cos_\\mathbf{x}\\theta_1 \\cos_\\mathbf{y}\\theta_2}{(\\mathbf{x}-\\mathbf{y})^2}\\)).21¬†Phase functions are normalized.\\[\n\\widehat{\\text{pdf}}(\\mathbf{x}_k) := \\begin{cases}\n\\displaystyle \\widehat{\\text{pdf}}_S(\\omega_i;\\mathbf{x}_k)=1 & \\mathbf{x}_k\\in S  \\\\\n\\displaystyle \\widehat{\\text{pdf}}_V(\\omega_i;\\mathbf{x}_k)=\\frac{1}{4\\pi} & \\text{otherwise}\n\\end{cases}\n\\]\nThis finally yields us the simplified expression below. 22\n22¬†Monte-Carlo estimation can be really confusing at times (in the context of importance sampling), just like the messy indices in Christoffel symbols or deriving backpropogation for optimizing function approximators. When you change the sampling process, you change the PDF in the single-sample formulation below, but you also implicitly change the factor when you average over the number of overall samples. The factor will reveal itself when working out the math. So, if we do cosine-weighted sampling for the surface scattering, yes the PDF will be \\(\\cos\\), yes it will cancel out the \\(\\cos\\) in the numerator, but \\(\\cos\\) will re-appear implicitly when you average over samples to ensure unbiasedness. This implictness appears as frequency.\\[\nI_j = \\frac{L_e(\\mathbf{x}_{1}\\to\\mathbf{x}_{2})G^*\\bigg[\\prod^{N-1}_{n=2}T(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n})\\hat{f}(\\mathbf{x}_{n-1}\\to\\mathbf{x}_{n}\\to\\mathbf{x}_{n+1})\\bigg]T(\\mathbf{x}_{N-1}\\to\\mathbf{x}_{N})W_e^{(j)}(\\mathbf{x}_{N-1}\\to\\mathbf{x}_{N})}{\\text{pdf}_A(\\mathbf{x}_1)(\\prod_{k=3}^{N-2} \\widehat{\\text{pdf}}(\\mathbf{x}_k)\\frac{1}{\\hat{t_k}})\\frac{1}{\\hat{t_N}}}\n\\]\n\\(G^*\\) since we are still doing area sampling on last vertex: the light.\nWe‚Äôve done it! As for the theory, there‚Äôs not much else to get this most basic version of volumetric path tracing running. Anything that comes after are more like algorithmic and design choices to make the code more optimal and fit nicely with the OptiX frameworks (e.g., closest hit, raygen, and miss hit shaders)."
  },
  {
    "objectID": "blog/10.html#algorithmic-view-of-monte-carlo-estimation-of-volumetric-path-integrals",
    "href": "blog/10.html#algorithmic-view-of-monte-carlo-estimation-of-volumetric-path-integrals",
    "title": "Foggy Cornell Box",
    "section": "Algorithmic View of Monte-Carlo Estimation of Volumetric Path Integrals",
    "text": "Algorithmic View of Monte-Carlo Estimation of Volumetric Path Integrals\n\n\n\n\n\n\nNoteAlgorithm: Basic Volumetric Path Tracing (Single-Sample)\n\n\n\nConstants: Scene geometry surfaces \\(\\mathcal{S}\\), isotrpoic single-scattering albedo \\(\\rho(\\mathbf{x},\\omega)\\), isotropic absorption coefficient \\(\\sigma_a(\\mathbf{x})\\), Russian Roulette \\(q\\), camera normal/direction \\(\\mathbf{n}\\)\nInput: Sensor position \\(\\mathbf{x}_p:=\\mathbf{x}_N\\), pre-determined initial incident direction \\(\\omega_i^{(1)}:=\\mathbf{x}_{N-1}\\to\\mathbf{x}_{N}\\)\nOutput: Outgoing radiance \\(L_o\\) (linear space)\n\n\\(\\sigma_s(\\mathbf{x})=\\frac{\\rho(\\mathbf{x})}{1-\\rho(\\mathbf{x})}\\sigma_a(\\mathbf{x})\\) // obtained by some algebra\n\\(\\sigma_t(\\mathbf{x}) \\triangleq \\sigma_a(\\mathbf{x})+\\sigma_s(\\mathbf{x})\\)\n\\(L_o = 0\\)\n\\(\\beta = |\\omega_i^{(1)}\\cdot\\mathbf{n}|\\) // path throughput (supports NEE) and sensor importance 24\n\\(\\mathbf{x} = \\mathbf{x}_p\\)\n\\(\\omega_i = \\omega_i^{(1)}\\)\nwhile True:\n\n\\(\\mathbf{y},\\mathbf{n}_\\mathbf{y} \\sim \\text{lightSurface}\\)\nif depth is \\(0\\): // direct lighting\n\nif \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) are mutually visible:\n\n\\(L_o \\mathrel{+}= L_ee^{-\\sigma_t||\\mathbf{y}-\\mathbf{x}||}\\)\n\n\n\\(t=\\text{rayTriangleIntersectionTest}(\\mathbf{x},-\\omega_i)\\)\n\\(u_q \\sim U[0,1)\\)\nif \\(u_q \\geq q\\): // Russian roulette\n\nbreak\n\n\\(\\beta \\mathrel{/}= q\\)\n\\(u \\sim U[0,1)\\)\nif \\(u &lt; 0.5\\): // do surface scattering\n\nif \\(t\\) is undefined: // ray miss\n\nbreak // there‚Äôs no \\(L_\\text{miss}\\) in volumetric light transport 25\n\n\\(\\mathbf{x}'=\\mathbf{x}-\\omega_i t\\)\n\\(\\omega_i \\sim \\text{cosWeightedBRDF}\\)\n\\(\\beta \\mathrel{*}= \\frac{\\rho}{\\pi}\\) // we assume Lambertian diffuse BSDF for now\n\\(\\beta \\mathrel{*}= e^{-\\sigma_tt}\\) // we assume homogenous medium for now26\n\nelse: // do volumetric scattering\n\nif \\(t\\) is undefined:\n\n\\(d \\sim \\text{Exp}(\\sigma_t)\\) // ray miss\n\\(\\beta \\mathrel{*}= \\frac{1}{\\sigma_t}\\) // PDF still has an extra factor\n\nelse:\n\n\\(d \\sim \\text{U}[0,t)\\)\n\n\\(\\beta \\mathrel{*}= te^{-\\sigma_td}\\)\n\n\\(\\mathbf{x}'=\\mathbf{x}-\\omega_i d\\)\n\\(\\omega_i \\sim \\text{sphericalPhase}\\)\n\\(\\beta \\mathrel{*}= \\sigma_s\\) // where the actual color scattering occur\n\\(\\beta \\mathrel{*}= 1\\) // \\(\\frac{\\frac{1}{4\\pi}}{\\frac{1}{4\\pi}}\\) by uniform sampling over spherical domain \n\nif \\(\\mathbf{y}\\) and \\(\\mathbf{x}'\\) are mutually visible:\n\n\\(G^*=\\frac{|\\mathbf{n}_\\mathbf{y}\\cdot (\\mathbf{x}'-\\mathbf{y})|}{(\\mathbf{x}'-\\mathbf{y})^2} e^{-\\sigma_t||\\mathbf{y}-\\mathbf{x}'||}\\) // geometry term with transmittance\n\\(L_o \\mathrel{+}= \\beta G^* L_e \\frac{1}{A}\\)\n\n\\(\\mathbf{x}=\\mathbf{x}'\\)\n\nreturn \\(L_o\\)\n\n\n\n26¬†Homogenous medium allows us to evaluate transmittance with tractable integral, allow us to analytically integrate the formulation to that.25¬†Despite us wanting to emulate foggy, it‚Äôs not like a hazy day or a foggy day where there‚Äôs a sun behind a thick layer of scattering, but think of it as foggy in the nighttime without moonlight or stars (i.e., it‚Äôs just goign to be pitch black).24¬†The sensor importance that corresponds to the physical process of vignetting. There‚Äôs an additional factor of \\(\\frac{1}{z^2}\\) where \\(z\\) is the normal distance between the pinhole aperature and the sensor surface/window (i.e., depth). Since majority of the cases \\(z=1\\), we can ignore it and leave just the \\(\\cos\\) term.I think I know how to live in harmony with AI coding tools now. Writing the algorithm to actually learn what you are doing without the slowdown of fixing errors, bugs, and dealing with syntax. Now, I can finally play around the coefficients in peace and have fun. (There‚Äôs not much to play around with this basic model.) As you play around more, you would realize the the ‚Äúgod‚Äù rays are very noise (i.e., high variance). There must be a better way and I hope I can improve it later down the line. For now, there‚Äôs other aspect of volumetric rendering I want to explore (i.e., inhomogenous medium and fluid simulation)."
  },
  {
    "objectID": "blog/10.html#visual-results",
    "href": "blog/10.html#visual-results",
    "title": "Foggy Cornell Box",
    "section": "Visual Results!",
    "text": "Visual Results!\n\n\n\nA slant view with a green single-scattering phase albedo and a lower absorption coefficient.\n\n\nNote: When I originally render the medium, it was pitch black. Due to how everything is ‚Äúfar apart‚Äù in my scene (i.e., on a order of 100s), the range of coefficients is much different. In fact, it has to be on a level of thousandths or ten-thousandths, and somewhat sensitive to changes in the coefficient. Just something to be aware.\n\n\n\nSpooky Cornell box (higher absorption).\n\n\n\nSource code: https://github.com/andrew-shc/vpt."
  },
  {
    "objectID": "blog/20.html",
    "href": "blog/20.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "You have probably seen this thing floating around in several machine learning courses: \\(P(y|x,\\theta)\\), \\(P(\\theta|x,y)\\), or even the term MAP and MLE. If you haven‚Äôt yet, you will probably start seeing them after. These concepts comes from statistics where they‚Äôre called statistical learning, parameter estimation, probabilistic inference, or one of their many synonyms. But what are they? How do they relate to vision? And why is it useful to think about them?\nLet us start with a toy example. We want to design an autonomous system to tell when the car should go or stop, depending on the traffic light. Our dataset are pictures of traffic lights and we want our model \\(M(\\theta)\\) to classify whether an image is \\(\\text{GO}\\) or \\(\\text{STOP}\\) parameterized by \\(\\theta\\). Using one-hot encoding, we let \\(\\mathbf{y}_i:=\\begin{bmatrix} 1 & 0 \\end{bmatrix}^\\top\\) for \\(\\text{GO}\\) and \\(\\mathbf{y}_i:=\\begin{bmatrix} 0 & 1 \\end{bmatrix}^\\top\\) for \\(\\text{STOP}\\). Let red and yellow light be \\(\\text{STOP}\\) and green light be \\(\\text{GO}\\) for our intents and purposes.\n\n\n\n1\n1¬†Image copied from here\n\nTo simplify our case even more, let us abstract away the high-dimensional input of an image and let \\(\\mathbf{x}_i:=\\begin{bmatrix} x_r & x_y & x_g \\end{bmatrix}^\\top\\), where each component \\(\\in[0,1]\\) corresponds to a snapshot of the illumination intensity of red \\(x_r\\), yellow \\(x_y\\), and the green \\(x_g\\) bulb of a traffic light. So, we can visualize our data as‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have introduced our scenario, let us get into the main idea of this blog."
  },
  {
    "objectID": "blog/20.html#maximum-likeilhood-estimation-mle",
    "href": "blog/20.html#maximum-likeilhood-estimation-mle",
    "title": "Statistical Learning",
    "section": "Maximum Likeilhood Estimation (MLE)",
    "text": "Maximum Likeilhood Estimation (MLE)\n\\(\\hat{\\theta}=\\arg\\max_{\\theta}p(\\mathbf{y}|\\mathbf{x},\\theta)\\)\nRemember, \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) are our training data and cannot be changed. The only thing we can (and should) change is the model parameter \\(\\theta\\). So, this is basically saying what value of \\(\\theta\\) can maximize the probability of the correct label \\(\\mathbf{y}\\) from its corresponding input \\(\\mathbf{x}\\) for all samples (i.e., data points). The \\(p(\\mathbf{y}|\\mathbf{x},\\theta)\\) is known as the likelihood of \\(\\theta\\). We will define what \\(\\theta\\) is later, but basically it is an abstract representation of model‚Äôs parameter, representing choices of hypothesis that best explains data relationship."
  },
  {
    "objectID": "blog/20.html#maximum-a-posteriori-map-estimation",
    "href": "blog/20.html#maximum-a-posteriori-map-estimation",
    "title": "Statistical Learning",
    "section": "Maximum a Posteriori (MAP) Estimation",
    "text": "Maximum a Posteriori (MAP) Estimation\n\\(\\hat{\\theta}=\\arg\\max_{\\theta}p(\\theta|\\mathbf{x},\\mathbf{y})=\\arg\\max_{\\theta}\\frac{p(\\mathbf{y}|\\mathbf{x},\\theta)p(\\theta)}{p(\\mathbf{x},\\mathbf{y})}\\propto \\arg\\max_{\\theta}p(\\mathbf{y}|\\mathbf{x},\\theta)p(\\theta)\\)\nThis is saying which parameter \\(\\theta\\) for our model (i.e., weights) has the highest probability that explains the data relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). In other words, this maximizes the posterior distribution \\(P(\\theta|\\mathbf{x},\\mathbf{y})\\) (i.e., the posterior of \\(\\theta\\)). We then use the Bayes‚Äô theorem 5 to get back our likelihood again with an additional prior \\(p(\\theta)\\) 6 with a constant factor \\(\\frac{1}{p(\\mathbf{x},\\mathbf{y})}\\) which can be ignored 7.\n5¬†A commonly used special operation in statistics/probabilities. Check here6¬†There‚Äôs many ways to think about prior \\(p(\\theta)\\). You think of it as a weighting factor of the likelihood that says how likely that likelihood is given the selected \\(\\theta\\). Or, as a balance between data-driven likelihood and previously known (i.e., ‚Äúprior‚Äù) knowledge on how \\(\\theta\\) should behave (e.g., \\(\\theta\\) is likely to be 67 for some reason). Or, as a regularizer where we follow Occam‚Äôs razor that the weights should be simple.7¬†This is a probability over the training data \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\). It‚Äôs not going to change throughout the maximization process of \\(\\theta\\)."
  },
  {
    "objectID": "blog/20.html#bayesian-inference",
    "href": "blog/20.html#bayesian-inference",
    "title": "Statistical Learning",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\\(p(y^*|x^*,\\mathbf{x},\\mathbf{y})=\\int p(y^*|x^*,\\theta)p(\\theta|\\mathbf{x},\\mathbf{y})d\\theta\\)\nInstead of estimating a specific parameter \\(\\theta\\) (i.e., point estimate), we interpret the model‚Äôs parameters \\(\\theta\\) as probabilistic 8. Notice we are not maximizing anything or even calculating \\(\\theta\\) itself, but directly predicting our unseen data point \\(y^*\\) given \\(x^*\\). In fact, this is a summation of each prediction \\(p(y^*|x^*,\\theta)\\) from all possible parameter \\(\\theta\\) weighted by the posterior for each \\(\\theta\\). The integral can sometimes be computed analytically given a good conjugate prior 9, but most of the time, it is approximated by sampling (i.e., Monte-Carlo).\n8¬†In statistics, there are two interpretations: frequentist (there‚Äôs a single true value and the randomness is strictly from sampling error like MLE/MAP) and Bayesian (there are no true value and the probability is intrinsic).9¬†A special prior when combined with its corresponding likelihood produces a nice-to-work-with analytical posterior (remember, we are integrating the posterior, which is nasty most of the time). For example, the Gaussian distribution \\(\\mathcal{N}(\\mu,\\sigma^2)\\)‚Äôs conjugate prior is Normal-inverse gamma. Check here."
  },
  {
    "objectID": "blog/20.html#others",
    "href": "blog/20.html#others",
    "title": "Statistical Learning",
    "section": "Others",
    "text": "Others\nFor generative or unsupervised models, we don‚Äôt necessarily have a label \\(\\mathbf{y}\\). What we do instead is have the model learn the data distribution of the input \\(\\mathbf{x}\\) itself, resulting in posteriors with \\(p(\\theta|\\mathbf{x})\\) instead of \\(p(\\theta|\\mathbf{x},\\mathbf{y})\\), or likelihood of \\(p(\\mathbf{x}|\\theta)\\) instead of \\(p(\\mathbf{y}|\\mathbf{x},\\theta)\\). Sometimes, they also have the latent distribution \\(\\mathbf{z}\\) as a more efficient, intermediate way to have the model learn the distribution. For example, VAEs Simon J. D. Prince (2023, chap. 17.3) and diffusions Simon J. D. Prince (2023, chap. 18.4) are learned via MLE with \\(p(\\mathbf{x}|\\theta)\\) as the likelihood. In some cases, we do a maximization and a minimization of two different sets of parameters \\(\\theta\\) and \\(\\phi\\) like in GANs Simon J. D. Prince (2023, chap. 15.1.1). Or, in Deep RL, we maximize the policy Simon J. D. Prince (2023, chap. 19.3).\nFor the purpose of learning (i.e., you learning this), we will stick with much more simpler MLE under supervised discriminative learning. Now enough with the theory and see some visuals from our example scenario."
  },
  {
    "objectID": "blog/20.html#analytical-estimation-of-theta",
    "href": "blog/20.html#analytical-estimation-of-theta",
    "title": "Statistical Learning",
    "section": "Analytical Estimation of \\(\\theta\\)",
    "text": "Analytical Estimation of \\(\\theta\\)\nSince GDA/QDA are clean to work with, they have an analytical method to compute the optimal parameter \\(\\theta\\) given the data \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). We basically compute the derivative of the log-likelihood with respect to each parameters to obtain the following 15.\n15¬†Check the derivations hereWe find that \\(\\phi_k=\\frac{n_\\mathbf{k}}{n}\\) where \\(n\\) is the total number of samples/data while \\(n_\\mathbf{k}\\) is the number of samples with \\(\\mathbf{y}_i=\\mathbf{k}\\).\n\\[\n\\begin{align}\n\\mathbf{\\mu}_\\mathbf{k} &= \\frac{\\sum_{i \\text{  s.t.  } \\mathbf{y}_i=\\mathbf{k}} \\mathbf{x}_i}{n_\\mathbf{k}} \\\\\n\\Sigma_\\mathbf{k} &= \\frac{\\sum_{i \\text{  s.t.  } \\mathbf{y}_i=\\mathbf{k}} (\\mathbf{x}_i-\\mathbf{\\mu}_\\mathbf{k})(\\mathbf{x}_i-\\mathbf{\\mu}_\\mathbf{k})^\\top}{n_\\mathbf{k}} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/20.html#gradient-descent-estimation-of-theta",
    "href": "blog/20.html#gradient-descent-estimation-of-theta",
    "title": "Statistical Learning",
    "section": "Gradient Descent Estimation of \\(\\theta\\)",
    "text": "Gradient Descent Estimation of \\(\\theta\\)\nSince we are using QDAs/GDAs, we have analytical solution available us (and we should use it in practice if QDAs/GDAs is what we really want). But most ML models, especially in vision, uses more complicated formulations like neural networks, we have to resort using numerical optimizations like gradient descent (GD), stochastic gradient descent, and Adam. So let us see how we optimize model in the majority of the cases via GD over the negative log-likelihood (NLL), which is just the negation of the log of the likelihood \\(\\frac{1}{n}\\sum^n_{i=1} p(\\mathbf{y}_i=\\mathbf{k}|\\mathbf{x}_i,\\theta)\\), including the NLL of the Gaussian in def neg_log_likelihood_gaussian. Note, we omit \\(\\log\\phi_\\mathbf{k}\\) since we are not optimizing that.\nThis will take about a few minutes. Make sure you run the previous cell.\n\n\n\n\n\n\nYou can see how the model distribution gets closer to the true data distribution for each gradient step. Remember, each point is a data point representing the 3-pixel image of a traffic light. Theoretically speaking, if there exists an analytical solution of a quadratic expression (single minimum), gradient descent should effectively reach to the same optimal point. However, the reason it‚Äôs not is likely because we‚Äôre missing the class prior \\(\\phi_\\mathbf{k}\\) or the variance (i.e., the spread) is different between the two categories (needing two different learning rate?) Regardless, it still mostly converges."
  },
  {
    "objectID": "blog/40.html",
    "href": "blog/40.html",
    "title": "Interesting Topics For November",
    "section": "",
    "text": "It‚Äôs always good to occasionally take a step back among the hustle and bustle, recalibrate your sense of direction, find new interesting questions to answer, and reflect the progress made from the past year. This blog aims to fossilize my interests at this time for me to potentially start a project from several of these and look back on, and maybe of interest for you too for whatever reason it may be. Note: this is more of an unorganized thought dump.\nMore computer graphics than 3D vision:\n\nPath-space differentiable rendering\nGeometric rendering: black holes\nRendering exotic light sources: cold-cathode displays\nStochastic Geometry\nRe-formulating problems as light-transport problems: Inverse rendering, walk-on-stars\n\nMore 3D vision than computer graphics:\n\nNeural 3D Reconstruction/Generation: VGGT, 3DGS, Diffusion\nDiffusion models & stochastic differential equations (SDEs)\nBundle Adjustment & Lie Algebra\nFast fourier transform (FFT)\nApplications of 3D Vision + graphics: Robotics, Perception, Reinforcement Learning"
  },
  {
    "objectID": "blog/40.html#path-space-differentiable-rendering-psdr",
    "href": "blog/40.html#path-space-differentiable-rendering-psdr",
    "title": "Interesting Topics For November",
    "section": "Path-space differentiable rendering (PSDR)",
    "text": "Path-space differentiable rendering (PSDR)\n\n\n\nImage from a course on physics-based differentiable rendering\n\n\nIn rendering, we take a description of a virtual 3D scene and have the renderer produce a 2D image of it given a camera location and orientation. More specifically, a scene has geometry (e.g., meshes, radiance fields) and light sources (or else the image would just be dark) where the renderer merely records the incoming radiance onto an image. So, an image is merely a record of the more complete virtual 3D scene that is easily transferred and displayed without needing to know the complicated underlying scene (e.g., *.ply files) and executing resource-intensive rendering process (e.g., GPUs). In fact, an image is not limited to recording virtual scenes, but it can also record real-life scenes where we usually don‚Äôt know the underlying complex environment (e.g., given a photo of a mountain, what is the geometry of the mountain?)1. This is where it gets interesting and how PSDR comes into play. Check the other section to see how PSDR is used in this context.\n1¬†But remember, a visual record needs light to mediate the underlying complex scene information. (Think exposure time: darker scenes needs longer time to capture the same amount of light to produce a sufficient visual record of a scene, whereas brighter scenes just needs shorter time. If a scene is completely dark, you can think of it needing infinite amount of time to produce an equal quality of a photo.)In forward rendering, there are mostly two common methods: rasterization and path tracing (and a mix of both). If we formalize these rendering process as a function, we get \\(I=f(\\theta)\\), where \\(I\\) is the final image, \\(\\theta\\) is the scene parameters (e.g., geometry, cameras), and \\(f\\) is the rendering process. What if we can differentiate it? Like \\(I=\\frac{d}{d\\theta}f(\\theta)\\)? Actually, this question is not too far-fetched in that machine learning has revolutionized in how we model intelligent-like behaviors and optimization, and differentiability is the core idea that enables such learning to take place.\nWe will be focusing on the more physically-plausible path tracing (via path-integral formulation). In this form, differentiating the rendering process produces two terms: interior term and boundary term. You can read more about them on [1], but basically interior term is what we are mainly differentiating against and the boundary term is an important term that accounts for visibility changes, which commonly appears when we differentiate against geometry.\nThis is why differentiation with respect to the geometry are messy and complicated. For geometric reconstruction, the shapes can change, inducing a visibility changes on a rendered image. For estimating camera pose, same thing: potential visibility changes. For moving geometries, same visibility changes. And what do we mean by visibilty changes anyways? When some geometry goes in front of another, we have an occlusion of the geometry in the back. These changes happen on discontinuities, making the differentiation hard. Hence, there‚Äôs a lot of current actively-explored topics on computing the derivatives in these fundamental areas.\nAnother important categories of parameter that we differentiate against are colors. More precisely, emission strength of light sources and the modulation by BSDF. No geometry means easier interior term and no boundary term, but that doesn‚Äôt mean it is easier.. overall. Sometimes, the light sources have small areas or the BSDF is highly-specular. In these cases, sampling becomes important and there are many active topics in this regards (e.g., variance reduction).\nOriginal paper on PSDR: [1]\n\n[1] C. Zhang, B. Miller, K. Yan, I. Gkioulekas, and S. Zhao, ‚ÄúPath-space differentiable rendering,‚Äù ACM Trans. Graph., vol. 39, no. 4, pp. 143:1‚Äì143:19, 2020.\nThere are other interesting questions on generalizing PSDR to different rendering processes and geometries:\n\nParticipating Media: [2]\nVolume Rendering: [3]\nother cool stuff‚Ä¶\n\n\n[2] C. Zhang, Z. Yu, and S. Zhao, ‚ÄúPath-space differentiable rendering of participating media,‚Äù ACM Trans. Graph., vol. 40, no. 4, pp. 76:1‚Äì76:15, 2021.\n\n[3] Z. Yu, C. Zhang, O. Maury, C. Hery, Z. Dong, and S. Zhao, ‚ÄúEfficient path-space differentiable volume rendering with respect to shapes,‚Äù Computer Graphics Forum, vol. 42, no. 4, 2023.\n\n[4] Y. Zeng, G. Cai, and S. Zhao, ‚ÄúA survey on physics-based differentiable rendering.‚Äù 2025. Available: https://arxiv.org/abs/2504.01402\nSurvey paper: [4]\nTopics I need to focus more on üòÑ:\n\nReparamaterizations\nSampling theory\nImplementing those scary differentiated version of those algorithms, especially when re-formulating it in terms of OptiX."
  },
  {
    "objectID": "blog/40.html#geometric-rendering-black-holes",
    "href": "blog/40.html#geometric-rendering-black-holes",
    "title": "Interesting Topics For November",
    "section": "Geometric rendering: black holes",
    "text": "Geometric rendering: black holes\n\n\n\nInterstellar (2014)\n\n\nWhen I mean geometric rendering, I specifically mean geometrically interesting light transport. Usually, rendering has always just been straight line, which serves majority of the purposes. And when we generalize the theory of light transport, we usually talk about wave 2 and quantum optics, according to [5], which is still just straight lines. Yes, mirrors, caustics, and diffraction does make it more interesting, but they still bend at a single point, not continuously. One thing pops up in my mind whenever I think of a light ray that continuously bends: black holes. Their force is so strong that it warps the light rays continuously, not just bend it at a single point.\n\n2¬†The dissertation mentioned how diffraction can occur even around an object. A good example of this is half-plane diffraction, where the lightened side is just bright, but the shadow side still exhibits extra radiance from the bending of the light or even wave-like oscillation under certain conditions. This is different from umbras and penumbras. Took a while to find an example of this effect since most diffraction examples are about apertures and slits, which I feel like is better termed as ‚Äúthrough an object.‚Äù\n[5] E. Veach, ‚ÄúRobust monte carlo methods for light transport simulation,‚Äù PhD thesis, Stanford University, Stanford, CA, USA, 1998.\n3¬†There also exists a similarly theory called special relativity, which is a specific case of general relativity. You can read more about here on ¬ß7 of the lecture notes. But basically we assume no gravity (hence, no force, no curvatures, and constant velocities) and focus on the spacetime nature of extreme cases in classical physics (e.g., perceived time at light speed).So, I guess a quick teaser (for me and you) with a demo, I will first create a point of singularity and a light path unaffected by it. What would a light path affected by gravitational force be like? We first need to know the theory behind gravity, and currently there are practically only two out there: Newton‚Äôs law (for most use cases) and general relativity 3 (for exotic cases). You can probably guess which theory contains the black hole.\nI started with ¬ß6: Black Holes, which says there are multiple kind of black holes with the Schwarzschild black hole being the simplest of all. However, this section mostly discuss about proving and understanding the various properties of many kinds of black holes, which will only be useful later on (beyond the scope). ¬ß1.3.5: Light Bending is what we care right now, discussing how light bends under Schwarzschild metric (i.e., the foundation for Schwarzschild black hole). By equation 1.54, we get the trajectory of our light ray affected by the gravitational force to be \\(\\frac{d^2u}{d\\phi^2}+u=\\frac{3GM}{c^2}{u^2}\\), where \\(u(\\phi)=\\frac{1}{r(\\phi)}\\) is the inverse polar radius, \\(G\\) is Newton‚Äôs gravitational constant (it appears again!), and \\(M\\) is the mass of our object warping the light path 4. I will quickly demo this by evaluating the ODE numerically via Euler‚Äôs method.\n4¬†Recall that light has no mass (i.e., null geodesic), so technically it is not orbiting around the black hole, but just following the spacetime frame itself that‚Äôs getting curved by the black hole.I first need to rewrite the second-order ODE \\(\\ddot{u}+u=\\frac{3GM}{c^2}{u^2}\\) (with \\(\\phi\\) as our independent variable) into a systems of first-order ODE for Euler‚Äôs method. \\[\n\\begin{cases}\n\\dot{u}=v \\\\\n\\dot{v}=\\frac{3GM}{c^2}{u^2}-u \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\nAdditional ideas:\n\nGravitational lensing (general case of black holes)\nWormholes\nCollision of two black holes\nRed shift\nRelativistic distortion (i.e., camera near the black holes)\nAccretion disk\nSpinning black holes\nCan we use differentiable rendering on an image of a black hole to estimate various parameter of it (e.g., mass)?\n\nJust out of curiosity (there‚Äôs definitely a better way)\n\n\nTopics to review:\n\nDifferential Geometry\nGeneral Relativity\nNon-euclidean geometry\nNumerical ‚Äúsolution‚Äù to ODE\n\nPersonal opinion: this is very different from the stuff I have done before and feels refreshing, but the math behind it is daunting at the same time.\nWhere graphics and astrophysics meet: paper link"
  },
  {
    "objectID": "blog/40.html#rendering-exotic-light-sources-cold-cathode-fluorescent-displays-ccfds",
    "href": "blog/40.html#rendering-exotic-light-sources-cold-cathode-fluorescent-displays-ccfds",
    "title": "Interesting Topics For November",
    "section": "Rendering exotic light sources: cold-cathode fluorescent displays (CCFDs)",
    "text": "Rendering exotic light sources: cold-cathode fluorescent displays (CCFDs)\n\n\n\nNixie Tubes\n\n\nI am a big fan of retro technologies, especially the warm neon lights of the bygone days. A year ago, I bought this (it‚Äôs a bit pricey) just out of fascination of what it looks like in real life. Unfortunately, it has this distinctive blue overtone which is missing when taking a photo with a consumer device (which makes it all of more interesting). Now, I‚Äôm curious if it‚Äôs possible to replicate this warm fuzzy feeling of a Nixie tube into computer rendering. There are three approaches to this with increasing level of complexity: emissive solid (i.e., a mesh with an emissive material), emissive participating media, or physically accurate simulation of CCFDs. I‚Äôll mostly be approaching it from the emissive participating media and a bit of physically-accurate modelling (i.e., physically-plausible CCFDs).\nSo how does one go about rendering the warm tone of CCFDs? Are they even always in an orange tone? How do they work? I‚Äôve came across this Google Group discussing about a book ‚ÄúCold Cathode Glow Discharge Tubes‚Äù, by GF Weston from 1968, available in PDF which has a wealth of information in how CCFDs work and their physical properties. Unfortunately, there were not a lot of discussion in optical properties (of this book and elsewhere I can find) that are important in rendering CCFDs. It is most likely because these electrical discharges (hence, light) from excited gasses produce all sorts of colors that are very sensitive to various parameters such as gas pressure, cathode-anode distance, voltage, amperage, gas composition, material used in for cathodes/anodes, etc. Since I‚Äôm not here to physically simulate what CCFDs would look like from these various parameters, I‚Äôll be focusing on Nixie tubes (which has more information usually) and maybe a limited range of parameters (it‚Äôs still interesting to see how CCFDs would evolve over certain parameters).\n\n\n\nFig 3.2 of ‚ÄúCold Cathode Glow Discharge Tubes‚Äù\n\n\nCCFDs can just be an emissive participating media with an assigned color of orange, but that‚Äôs no fun, since there are also various color overtones it produces depending on the gas composition that would be missing. Hence, I am having the input to just be the gas composition and some basic geometric model of the energy field (i.e., the emission surrounding the shape of the cathode or the cathode sheathed in a layer of emission5). Unfortunately, it seems like most Nixie Tube discussion lack the information on their visual properties. So, we turn to the physics of light in hopes of simulating the same physically correct emission of gas composition like shown in the picture. To get there, we need to know some radiative transfer theory, which is readily talked about in [6, Ch. 11] for volumetric scattering process and more completely in [6, Ch. 14] with the introduction of the radiative transfer equation.\n5¬†Sometimes, the negative glow is shown sticking onto the cathode. In other times, we have two dark spaces with three regions of emission from the cathode side (before the Faraday dark space). The geometries of emission can be quite sensitive, so we will assume the ‚Äúnegative glow‚Äù is sticked/sheathed onto the cathode.6¬†As I look into this & the Nixie Tubes more, there are many more physical abstraction you can uncover (e.g., electrical glow discharge, fluorescent bulbs, anisotropic absorption/out-scattering, etc.), which at a certain point, you‚Äôre just modelling electromagnetic waves themselves. In my opinion, we model the most physically-grounded renderer as much as possible while also keeping it fast, so given some known discrete scene parameters, we can render a realistic image in a computationally-sensible way. I hope this is what graphics is all about.In computer graphics, rendering volumetric effects requires us to at least model three main 6 physical process: absorption, emission, and scattering (further broken down as in-scattering and out-scattering). By assumption, we model these effects as differential equations (ordered as emission, absorption, out-scattering, and in -scattering):\n\\[\n\\begin{align}\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= \\sigma_a(\\mathbf{x},\\mathbf{\\omega}) L_e(\\mathbf{x},\\mathbf{\\omega})dt \\\\\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= -\\sigma_a(\\mathbf{x},\\mathbf{\\omega}) L_i(\\mathbf{x},-\\mathbf{\\omega})dt \\\\\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= -\\sigma_s(\\mathbf{x},\\mathbf{\\omega}) L_i(\\mathbf{x},-\\mathbf{\\omega})dt \\\\\ndL_o(\\mathbf{x},\\mathbf{\\omega}) &= \\sigma_s(\\mathbf{x},\\mathbf{\\omega})\\int_{S^2} p(\\mathbf{x},\\mathbf{\\omega},\\mathbf{\\omega}_i)L_i(\\mathbf{x},\\omega_i)d\\omega_i dt \\\\\n\\end{align}\n\\]\nwhere \\(dL_o(\\mathbf{x},\\mathbf{\\omega}):=L_o(\\mathbf{x}+\\mathbf{\\omega}dt, \\mathbf{\\omega})\\) and \\(L_o(\\mathbf{x},\\mathbf{\\omega}):=L_i(\\mathbf{x},-\\mathbf{\\omega})\\) (remember that we are only traversing the volumetric media in a straight line to simplify our model).\n\\(\\sigma_a\\) is the absorption coefficient and \\(\\sigma_s\\) is the extinction coefficient, and \\(p\\) is the phase function‚Äîthe BSDF reciprocal of volumetric scattering.\nSo, if we rewrite the formulas, the combined differential change in radiance at a point \\(\\mathbf{p}'=\\mathbf{p}+t\\mathbf{\\omega}\\) in a volumetric media is \\(\\frac{d}{dt}L_o(\\mathbf{p}',\\mathbf{\\omega})=-\\sigma_t(\\mathbf{p}',\\mathbf{\\omega})L_i(\\mathbf{p}',-\\mathbf{\\omega})+\\sigma_t(\\mathbf{p}',\\mathbf{\\omega})L_s(\\mathbf{p}',\\mathbf{\\omega})\\). The \\(\\sigma_t=\\sigma_a+\\sigma_s\\) is the extinction/attenuation coefficient, and in this case it can be interpreted as the medium‚Äôs density, where larger value means more effect from the medium while lower value means less effect (i.e., transparent air). Notably, the first term includes the transmittance, which can be analytically 7 solved into \\(e^{-\\sigma_t d}\\) where \\(d\\) is the distance 8. Additionally, in [6], Equation 11.10, they mysteriously used the indirect \\(T_r(\\mathbf{p}+t\\omega\\to\\mathbf{p}')\\) instead of the more direct \\(T_r(\\mathbf{p}\\to\\mathbf{p}+t\\omega)\\), but are equivalent with a change of variable. This can be thought of finding the optical depth/thickness from \\(0\\to t\\) or remainder of the optical thickness from \\(t \\to d\\) (the inner transmission integral).\n7¬†Assuming homogenous media (i.e., constant density).8¬†This is better known as the exponential volumetric transport, which is commonly seen in NeRF and NeuS. A natural question to ask is whether non-exponential volumetric transport exist? In fact, it does. The volumetric transport we, and the book, implicitly assume is the Beer-Lambert law., whereas non-exponential version opens a whole can of worms. Both all are governed by the generalized radiative transfer equation (RTE).\n[6] M. Pharr, W. Jakob, and G. Humphreys, Physically based rendering, fourth edition: From theory to implementation. MIT Press, 2023. Available: https://books.google.ad/books?id=kUtwEAAAQBAJ\n9¬†As usual, we assume each \\(N\\)-length path has a single emission source at the end, but \\(N\\) can vary from various sampled paths. Just makes the math cleaner without restricting anything.To actually render volumetric effect, we need a volumetric integrator that generalizes the volumetric scattering effect as a path -integral formulation. The general idea is simple: we just generalize the formulation where the domain include medium locations instead of just surface locations and the measure extended to measure positions of volumetric effect. The contribution function is extended to support phase functions (i.e., in-scattering effects), transmittance (i.e., absorption and out-scattering), and emission 9.\n\nAs I‚Äôm writing this out, I also realized clouds is another form volumetric media I want to see it work. Not just a static cloud, but like maybe some procedural noise or even a simple fluid simulation going to see the clouds grow and shrink over time.\nAdditional ideas:\n\nFluid simulation of clouds?\nAccurately render the emission spectrum described from a molecular composition?\n\nTopics to review:\n\nRadiative transfer equation (RTE)\nBeer‚Äôs Lambert Law\nFluid dynamics\nSampling volumetric light transports\n\nPersonal opinion: I think this follows more on traditional computer graphics for good reason, volumetric media is common and knowing it is useful (and it‚Äôs also fun).\nOriginally wanted to model some exotic light sources, but realized they are quite a complex physical process. This section (i.e., the topic) is probably more aptly named as volumetric light transport. Good thing I did this so I have a better idea what the project could look like."
  },
  {
    "objectID": "blog/40.html#stochastic-geometry",
    "href": "blog/40.html#stochastic-geometry",
    "title": "Interesting Topics For November",
    "section": "Stochastic geometry",
    "text": "Stochastic geometry\nAs if probability (or this notion of probabilistic stuff) wasn‚Äôt enough, stochasticity (or stochastic stuff) is this new kid down the street I have been hearing quite frequently (apart from stochastic gradient descent). Let us answer first on what is stochasticity, and how is it different from ‚Äúprobabilistic‚Äù?\nAccording to Merriam-Webster, for something to be stochastic, it means the something involves a random variable, chance, or probability, which actually is just ever more slightly specific then the term ‚Äúprobabilistic.‚Äù In other words, they‚Äôre quite same. However, from my initial understanding, people generally associate probabilistic model as something that emphasizes more on learning and inference, whereas stochastic model emphasizes more on modelling the randomness (to a certain degree) of what otherwise would be a mostly deterministic evolutionary process over time, space, or any other indexable/ordered objects (i.e., stochastic process).\nNow the phrase ‚Äústochastic geometry‚Äù should be a bit clearer now, let‚Äôs just dive what‚Äôs all the buzz with this in these two papers: [7] and [8]. 10 Remember, we should always be asking ourselves whether this ‚Äústochastic geometry‚Äù is bringing us benefits in one or both aspects while maintaining at least one other aspect: computationally practicality (on current or near future processors) and physically-correct appearances 11.\n10¬†Actually, the more I‚Äôm reading this two papers, the more I am grateful that I have covered volumetric light transport firstüòÖ. (Hinting on whats coming up.)11¬†If we only want physically-correct appearances, we might as well have all the supercomputers in the world to handle all the complex physical phenomenon (which actually uses a ton of statistics for a long time already). If we only want computational practicality, we have rasterization (e.g., vanilla Minecraft, Roblox, Terraria, etc.).\n\n\nFig 1. from [7]\n\n\n[7] focused on generalizing the exponential volumetric transport to solid media to explain how recent works in signed distanced fields 12 have been successful in modelling opaque, solid geometries as stochastic participating media.\n12¬†SDFs requires some sort of volumetric transport to be rendered directly. There are also marching cubes to convert SDFs into meshes.\n\n\nFrom [8]\n\n\nTheir novelty is mainly in developing an efficient rendering algorithm for a new type of scene representations. While this efficient algorithm is lathered with technical details, the background of their work is interesting: bringing stochasticity into scene geometry to unify all representations into one, which traditionally has a collection of specialized models to render certain geometries (usually surfaces and participating media). This new generalization naturally gives us geometries that are in between the participating media and surfaces known as ‚Äúmesoscale‚Äù geometries. I hope this allows us to additionally model certain appearances in the physical world that would otherwise be difficult (i.e., larger parameter space that fits closer to physically-correct models).\nWhile both papers uses the same particular interpretation of stochastic geometry, known as stochastic implicit surfaces (SIS) via signed distance functions (SDF), [7] emphasizes on one particular case where opaque solids are represented as stochastic participating media due to the rise of NeuS (and indirectly via NeRF), whereas [8] attempts to further generalize all classes of geometries onto a continuum that allows exploration of new materials (that could potentially be useful for physically-correct geometric modelling).\n\n[7] B. Miller, H. Chen, A. Lai, and I. Gkioulekas, ‚ÄúObjects as volumes: A stochastic geometry view of opaque solids,‚Äù in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), 2024, pp. 87‚Äì97.\n\n[8] D. Seyb, E. d‚ÄôEon, B. Bitterli, and W. Jarosz, ‚ÄúFrom microfacets to participating media: A unified theory of light transport with stochastic geometry,‚Äù ACM Transactions on Graphics (Proceedings of SIGGRAPH), vol. 43, no. 4, Jul. 2024, doi: 10/gt5nh9.\nTopics to review:\n\nVolumetric light transports\nStochastic process\n\nPersonal opinion: This seems interesting to me where we incorporate randomness into opaque solid geometries. In reality, this idea has sort of been implicitly used in a lot of places already (e.g., NeuS) and mostly serves as a theoretical interest. But if we see stochastic geometry as a way to generalize material models, I think this is something cool to see it work on a toy example."
  },
  {
    "objectID": "blog/40.html#re-formulating-problems-as-light-transport",
    "href": "blog/40.html#re-formulating-problems-as-light-transport",
    "title": "Interesting Topics For November",
    "section": "Re-formulating problems as light-transport",
    "text": "Re-formulating problems as light-transport\nIt is always interesting to see how other problem domains can be reduced to a light transport problem (or to any problem in general), since you get to explore and understand what are the fundamental properties of light transports that would be useful to find the solution. Additionally, you would probably have to figure out what are the underlying assumption needed to enables these structure for finding potential solutions. Currently, there are two notable domains of problems that fits well into light-transport: inverse rendering (via differentiable rendering) and PDE solvers.\n\nInverse Rendering\nRendering \\(f\\) is all about capturing lights from known scene description \\(\\theta\\) into images \\(I\\) right 13? \\(I=f(\\theta)\\)\n13¬†As mentioned in the first sectionCamera \\(C\\) is all about capturing lights via real physical process from unknown scene description \\(\\phi\\) into images \\(I'\\) right? \\(I'=C(\\phi)\\)\nWhat if we want to find the unknown \\(\\phi\\)?\nIf \\(f\\approx C\\) 14, we can minimize the difference between the two images via optimization to find the optimal scene description \\(\\theta\\) that best estimates \\(\\phi\\), where \\({\\arg\\min}_\\theta I-I'={\\arg\\min}_\\theta f(\\theta)-C(\\phi)\\). This requires differentiating the rendering process with respect to \\(\\theta\\), allowing us to piggy-back on years of development on tools developed from forward physically-plausible rendering into inverse rendering.\n14¬†In other words, we assume that our synthetic rendering process \\(f\\) is able to model most light phenomenon accurately, almost like \\(C\\).\n\nPDE solvers\nWhenever I think of solving anything about differential equations, I just think of the common examples given from classes: Newton‚Äôs law of cooling or the logistic growth models (maybe for good reason??). But in PDEs, solutions are much more interesting to think about and visualize since a large part of is solving them numerically15. Common methods includes finite element method (FEM) and boundary element method (BEM), but they all have tradeoffs, especially when we want to query a value in the interior. However, we can use Walk on Spheres to find the value more efficiently from the principles of light transport and computer graphics.\n15¬†Actually, there‚Äôs some interesting analytical solutions and analysis in ODEs/PDEs such as Laplace and Fourier transformation.\n[9] R. Sawhney and B. Miller, ‚ÄúMonte carlo geometry processing,‚Äù in SGP 2024 graduate school courses, 2024.\nThere‚Äôs an online course that talks about this more in-depth: [9].\n\n\nAny others?\nIt is likely that I will have to play around other orthogonal subjects that interests me and see if there are any problems in there that can be reduced to a light transport."
  },
  {
    "objectID": "blog/40.html#neural-3d-reconstructiongeneration",
    "href": "blog/40.html#neural-3d-reconstructiongeneration",
    "title": "Interesting Topics For November",
    "section": "Neural 3D Reconstruction/Generation",
    "text": "Neural 3D Reconstruction/Generation\n3DGS sounded very cool at first, but you would realize soon enough that they are more for novel-view synthesis, because at the end of the day, they are still a point cloud. Though this is changing fast also. From their pipeline, I guess one thing that still intrigues me is how they do the adaptive density control. A lot of them are heuristics, but two main ones are under-reconstruction and over-reconstruction, both derived from view-space positional gradients (i.e., what direction should the original gaussian move to minimize the error). If the gaussian is small and in under-reconstructed region, they just make a clone and move it along the original gaussian splat‚Äôs view-position gradient. If the gaussian has a high-variance and is in over-reconstructed region, then just use the original gaussian as PDF for sampling the next two gaussian‚Äôs position with smaller size.\nThe problem of Structure from Motion is how can we estimate camera poses and world geometry (usually point clouds) just from images? Remember, a lot models relies on this for their training, such as NeRFs and 3DGS. Hence, it is of great interest.\n\nTraditional (old): COLMAP\nMixed (new): Dust3r, Mast3r, VGGSfM\nFull ML (latest): VGGT\n\nWith the recent release of VGGT, it is awesome to see 3D reconstruction working purely from a feed-forward network with minimal pre-processing and post-processing.\nAnother intriguing problem are generative models that generate 3D data, especially from a single view and via diffusion. I will have to look more papers about 3D estimation from a single-view."
  },
  {
    "objectID": "blog/40.html#diffusion-models-stochastic-differential-equations-sdes",
    "href": "blog/40.html#diffusion-models-stochastic-differential-equations-sdes",
    "title": "Interesting Topics For November",
    "section": "Diffusion Models & Stochastic Differential Equations (SDEs)",
    "text": "Diffusion Models & Stochastic Differential Equations (SDEs)\nThe general idea is that this is a generative model that is able to spatially generate realistic 2D images unconditionally or conditionally (e.g., text prompt, image prompt, etc.). It does this clever trick where it repeatedly adds noise to the training images until it becomes indistinguishable to fully random images, but the model is trained on these noises to denoise them later. So on the next time (i.e., inference-time), you feed it a random image and it will denoise it to look realistic learnt from the training dataset. There are many great resources to learn diffusion models now. In particular, if I do get the time, I plan to read [10, Ch. 17‚Äì18] and the original DDPM paper.\n\n[10] S. J. D. Prince, Understanding deep learning. The MIT Press, 2023. Available: http://udlbook.com\nIn the most recent survey paper on diffusion models here, there‚Äôs an interesting connection between the diffusion process and SDEs. ODEs/PDEs are useful when modeling from a set of rules (i.e., equations) of a dynamic system, which we solve the true underlying function analytically or numerically via simulations. Sometimes, these dynamic systems can have some random process (e.g., diffusion noise) term incorporated, resulting in stochastic differential equations.\nAdditional ideas:\n\nDiffusion models on 3D geometries\nCode or video animation of SDEs (since they‚Äôre inherently an evolutionary process of a dynamic system, this is probably the best way for visualization)\n\nTopics to review:\n\nStochastic process\nStochastic differential equations\nStochastic calculus"
  },
  {
    "objectID": "blog/40.html#bundle-adjustment-ba-lie-algebra",
    "href": "blog/40.html#bundle-adjustment-ba-lie-algebra",
    "title": "Interesting Topics For November",
    "section": "Bundle Adjustment (BA) & Lie Algebra",
    "text": "Bundle Adjustment (BA) & Lie Algebra\nI have always thought Bundle Adjustment was small, simple thing in SfM16, until I started coming across in the original ORB-SLAM paper and mentions some foreign concepts such as Lie Algebra. Lie (pronounced Lee) Algebra is interesting in that it has a geometric intuition from differential geometry, but also has an associated Lie Group (from group theory). However, Lie Algebra is commonly talked in relation to robotics state estimation in the book and the paper. Regardless, these are some pointers that seems to be a good resource to get started.\n16¬†It‚Äôs the last process that goes by pretty fast usually. And this idea of ‚Äúadjustment‚Äù didn‚Äôt really do some justice since it just evokes a small algorithm that nudges parameter to achieve something that‚Äôs a tad bit more optimal to make the estimations more polished and nice (which it does, but much more complex).For bundle adjustment, the key idea is that we already have a rough estimation from previous estimates (or just randomly initialize it a super bad state). Then, BA attempts to globally adjust and refine both the camera pose and point locations to produce a higher quality reconstruction. This classical survey talks how one implements BA in-depth, while this modern survey attempts to optimize BA in a distributed manner.\nAdditional ideas:\n\nCode or video animation of the optimization process\n\nTopics to review/prepare:\n\nNon-linear optimization\n\nLevenberg‚ÄìMarquardt\n\nLots of linear algebra tricks\nLie algebra + groups (why not)"
  },
  {
    "objectID": "blog/40.html#fast-fourier-transform-fft",
    "href": "blog/40.html#fast-fourier-transform-fft",
    "title": "Interesting Topics For November",
    "section": "Fast Fourier Transform (FFT)",
    "text": "Fast Fourier Transform (FFT)\n\n\n\n[11, Fig. 16.8]\n\n\nFirst of all, what is fourier transform? It converts raw signals (i.e., time domain) into an amplitude function of frequency (i.e., frequency domain). In the [11, Ch. 15‚Äì16], they talk more rigorously on how it works, specifically discrete fourier transform in the context of spatial 2D image signals.\n\n[11] A. Torralba, P. Isola, and W. T. Freeman, Foundations of computer vision. in Adaptive computation and machine learning series. MIT Press, 2024. Available: https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/\nFast fourier transform is an algorithm to automatically find the transformed signals automatically. It will be interesting if I just implement my own FFT and see the various frequency that automatically pops out and an attempt to reconstruct it signal by signal.\nThis also reminds of spherical harmonics. Since they also use complex exponentials, maybe there‚Äôs some connection there?\nTopics to review/prepare:\n\nDiscrete fourier transform\nFFT algorithm"
  },
  {
    "objectID": "blog/40.html#applications-of-3d-vision-graphics",
    "href": "blog/40.html#applications-of-3d-vision-graphics",
    "title": "Interesting Topics For November",
    "section": "Applications of 3D Vision + Graphics",
    "text": "Applications of 3D Vision + Graphics\n\n\n\nWorld model. Sourced from here\n\n\nOne particular applications of 3D vision and graphics that is interesting right now is visuomotor and generative world models, combining all three disciplines from robotics (specifically autonomous agents) and perception (i.e., vision with sensor fusion) to reinforcement learning. The applications for both of them definitely leans more on the frontier/novel side of things, but it would be interesting to see it used in more classical settings (because why not)."
  }
]